{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import all libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import zipfile\n",
    "import requests\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "import fasttext as ft\n",
    "import fasttext.util as ftutil\n",
    "import numpy as np\n",
    "import re\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from tqdm import tqdm\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.preprocessing import normalize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement download functions to download the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_and_extract(url, zip_path, extract_path):\n",
    "    if os.path.exists(extract_path):\n",
    "        print(f\"File {extract_path} already exists.\")\n",
    "    else:\n",
    "        print(f\"Downloading and extracting the {zip_path} to {extract_path}.\")\n",
    "        response = requests.get(url, stream=True)\n",
    "        total_size = int(response.headers.get('content-length', 0))\n",
    "        block_size = 1024\n",
    "        with open(zip_path, 'wb') as file, tqdm(\n",
    "            desc=zip_path,\n",
    "            total=total_size,\n",
    "            unit='iB',\n",
    "            unit_scale=True,\n",
    "            unit_divisor=1024,\n",
    "        ) as bar:\n",
    "            for data in response.iter_content(block_size):\n",
    "                file.write(data)\n",
    "                bar.update(len(data))\n",
    "        if not os.path.exists(extract_path):\n",
    "            os.makedirs(extract_path)\n",
    "        with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "            for member in zip_ref.namelist():\n",
    "                member_path = os.path.join(extract_path, os.path.relpath(member, start=os.path.commonpath(zip_ref.namelist())))\n",
    "                if member.endswith('/'):\n",
    "                    os.makedirs(member_path, exist_ok=True)\n",
    "                else:\n",
    "                    os.makedirs(os.path.dirname(member_path), exist_ok=True)\n",
    "                    with open(member_path, 'wb') as f:\n",
    "                        f.write(zip_ref.read(member))\n",
    "        os.remove(zip_path)\n",
    "        print(f\"Download and extraction of {extract_path} complete.\")\n",
    "\n",
    "def download(url, file_path):\n",
    "    if os.path.exists(file_path):\n",
    "        print(f\"File {file_path} already exists.\")\n",
    "    else:\n",
    "        print(f\"Downloading the file {file_path}\")\n",
    "        response = requests.get(url, stream=True)\n",
    "        total_size = int(response.headers.get('content-length', 0))\n",
    "        block_size = 1024\n",
    "\n",
    "        with open(file_path, 'wb') as file, tqdm(\n",
    "            desc=file_path,\n",
    "            total=total_size,\n",
    "            unit='iB',\n",
    "            unit_scale=True,\n",
    "            unit_divisor=1024,\n",
    "        ) as bar:\n",
    "            for data in response.iter_content(block_size):\n",
    "                file.write(data)\n",
    "                bar.update(len(data))\n",
    "        print(f\"Download of {file_path} complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wikipedia dataset\n",
    "wiki_url = \"https://www.kielipankki.fi/download/wikipedia-fi/wikipedia-fi-2017-src/wikipedia-fi-2017-src.zip\"\n",
    "wiki_zip_path = \"wikipedia-fi-2017-src.zip\"\n",
    "wiki_extract_path = \"data/wikipedia-fi-2017-src\"\n",
    "download_and_extract(wiki_url, wiki_zip_path, wiki_extract_path)\n",
    "\n",
    "# Finnish Stopwords\n",
    "stopword_url = \"http://members.unine.ch/jacques.savoy/clef/finnishST.txt\"\n",
    "stopwords_path = \"data/finnishST.txt\"\n",
    "download(stopword_url, stopwords_path)\n",
    "\n",
    "# Finnish Stemmer\n",
    "stemmer_url = \"http://members.unine.ch/jacques.savoy/clef/finnishStemmer.txt\"\n",
    "stemmer_path = \"data/finnishStemmer.txt\"\n",
    "download(stemmer_url, stemmer_path)\n",
    "\n",
    "# FinnWordNet\n",
    "finnwordnet_url = \"https://www.kielipankki.fi/download/FinnWordNet/v2.0/FinnWordNet-2.0.zip\"\n",
    "finnwordnet_zip_path = \"FinnWordNet-2.0.zip\"\n",
    "finnwordnet_extract_path = \"data/FinnWordNet\"\n",
    "download_and_extract(finnwordnet_url, finnwordnet_zip_path, finnwordnet_extract_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1:\n",
    "\n",
    "Consider the wordings: “climate change”, “emission”, “resilience”, “sustainability” (need to consider their Finnish translations). Suggest a script where your input each of these wordings and output the corresponding Wikipedia pages, highlighting all the (linked) entities in these pages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "terms = {\n",
    "    'ilmastonmuutos',\n",
    "    'päästö',\n",
    "    'joustavuus',\n",
    "    'ekologinen kestävyys'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load the extracted Wikipedia dataset\n",
    "def load_wikipedia_dataset(directory, max_print=5):\n",
    "    data = {}\n",
    "\n",
    "    # Iterate over all dataset parts\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.startswith(\"wiki_part\") and filename.endswith(\".VRT\"):\n",
    "            file_path = os.path.join(directory, filename)\n",
    "            print(f\"Processing file: {file_path}\")\n",
    "\n",
    "            with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                content = file.read()\n",
    "\n",
    "                # Use regex to find all documents within the <doc>...</doc> tags\n",
    "                docs = re.findall(r'<doc id=\"(.*?)\" url=\"(.*?)\" title=\"(.*?)\">(.*?)</doc>', content, re.DOTALL)\n",
    "\n",
    "                # Process each document found\n",
    "                for doc_id, url, title, doc_content in docs:\n",
    "\n",
    "                    # Only store the data if the title matches one of the specified terms\n",
    "                    if title.casefold() in terms:\n",
    "                        data[title.casefold()] = {\n",
    "                            'id': doc_id,\n",
    "                            'url': url,\n",
    "                            'content': doc_content\n",
    "                        }\n",
    "    return data\n",
    "\n",
    "\n",
    "# Load Finnish stopwords\n",
    "def load_stopwords(file_path):\n",
    "    with open(file_path, 'r', encoding='ISO-8859-1') as f:\n",
    "        stopwords = f.read().splitlines()\n",
    "        print(f\"Loaded {len(stopwords)} stopwords from {file_path}\")\n",
    "    return stopwords\n",
    "\n",
    "# Function to highlight linked entities\n",
    "def highlight_linked_entities(content):\n",
    "    highlighted_content = re.sub(r'(<link entity=\")(.*?)(\">)(.*?)(</link>)', r'\\1\\2\\3**\\4**\\5', content)\n",
    "    return highlighted_content\n",
    "\n",
    "# Function to tokenize text into sentences and remove stopwords\n",
    "def remove_stopwords(sentence, stopwords, language = 'finnish'):\n",
    "    words = word_tokenize(text = sentence, language = language)\n",
    "    filtered_words = [word for word in words if word.lower() not in stopwords]\n",
    "    return ' '.join(filtered_words)\n",
    "\n",
    "# Function to remove '#' from the middle of a word\n",
    "def remove_hash_from_words(text):\n",
    "    return re.sub(r'(\\w)#(\\w)', r'\\1\\2', text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the wikipedia dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_directory = \"data/wikipedia-fi-2017-src\"  # Path to the extracted dataset\n",
    "wikipedia_data = load_wikipedia_dataset(dataset_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the finnish stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "finnish_stopwords = load_stopwords(stopwords_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Highlight the entities in the wikipedia pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "for title, info in wikipedia_data.items():\n",
    "    highlighted_content = highlight_linked_entities(info['content'])\n",
    "    #print(f\"Title: {title}\\nHighlighted Content: {highlighted_content}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the pages for the given wordings and get the sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract third column data from sentences\n",
    "def extract_third_column(sentence):\n",
    "    third_column_data = []\n",
    "    lines = sentence.strip().split('\\n')\n",
    "    for line in lines:\n",
    "        if line.strip() and not line.startswith('<'):\n",
    "            columns = line.split('\\t')\n",
    "            if len(columns) > 2:\n",
    "                third_column_data.append(columns[2])\n",
    "    return third_column_data\n",
    "\n",
    "def process_paragraphs(input_text):\n",
    "    #print(f\"Input Text: {input_text}\")\n",
    "    paragraphs = re.findall(r'<paragraph>(.*?)</paragraph>', input_text, re.DOTALL)\n",
    "    all_third_column_data = []\n",
    "    #print(f\"Paragraphs: {paragraphs}\")\n",
    "    for paragraph in paragraphs:\n",
    "        sentences = re.findall(r'<sentence>(.*?)</sentence>', paragraph, re.DOTALL)\n",
    "        #print(f\"Sentences: {sentences}\")\n",
    "        for sentence in sentences:\n",
    "            #print(f\"Sentence: {sentence}\")\n",
    "            third_column_data = extract_third_column(sentence)\n",
    "            all_third_column_data.append(' '.join(third_column_data))\n",
    "    return all_third_column_data\n",
    "\n",
    "#print(f\"Wikipedia data: {wikipedia_data['ilmastonmuutos']['content']}\")\n",
    "\n",
    "# Process each item in wikipedia_data\n",
    "for item in wikipedia_data.items():\n",
    "    third_column_data = process_paragraphs(item[1]['content'])\n",
    "    print(f\"Item {item[0]} third column data:\")\n",
    "    for j, data in enumerate(third_column_data):\n",
    "        filtered_sentence = remove_stopwords(data, finnish_stopwords)\n",
    "        print(f\"Sentence {j+1}: {filtered_sentence}\")\n",
    "        # Save the filtered sentence to a variable if needed\n",
    "        # For example: sentence_var = filtered_sentence\n",
    "    print(\"\\n\" + \"#\" * 80 + \"\\n\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "sentences = re.findall(r'<sentence>(.*?)</sentence>', data['ilmastonmuutos']['content'], re.DOTALL)\n",
    "all_words = []\n",
    "\n",
    "if sentences:\n",
    "    for idx, sentence_content in enumerate(sentences):\n",
    "        print(f\"Processing sentence {idx + 1}...\")\n",
    "\n",
    "        # Split each sentence into individual lines\n",
    "        lines = sentence_content.strip().split('\\n')\n",
    "\n",
    "        # Extract the word (3nd column) from each line\n",
    "        words = [line.split('\\t')[2] for line in lines if line.strip()]\n",
    "        all_words.extend(words)\n",
    "else:\n",
    "    print(\"No sentences found in the document.\")\n",
    "\n",
    "print(f\"Words: {all_words}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the Finnish Wikipedia 2017 dataset, Finnish Stopwords, Stemmer and FinnWordNet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2\n",
    "\n",
    "Assume the content of each webpage is a single document. Use relevant NLTK script to create a corpus constituted of the four document, and appropriate proprocessing and lemmatization, to construct the TfIdfVectorizer of each document and then calculate the cosine similarity of each pair of these documents. Provide the result in a table and comment on the findings."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
