{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import all libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import zipfile\n",
    "import requests\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "import fasttext as ft\n",
    "import fasttext.util as ftutil\n",
    "import numpy as np\n",
    "import re\n",
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from tqdm import tqdm\n",
    "from nltk.corpus.reader.wordnet import WordNetCorpusReader\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.preprocessing import normalize\n",
    "from wnaffect import WNAffect\n",
    "from emotion import Emotion\n",
    "from googletrans import Translator\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement download functions to download the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_and_extract(url, zip_path, extract_path):\n",
    "    if os.path.exists(extract_path):\n",
    "        print(f\"File {extract_path} already exists.\")\n",
    "    else:\n",
    "        print(f\"Downloading and extracting the {zip_path} to {extract_path}.\")\n",
    "        response = requests.get(url, stream=True)\n",
    "        total_size = int(response.headers.get(\"content-length\", 0))\n",
    "        block_size = 1024\n",
    "        with open(zip_path, \"wb\") as file, tqdm(\n",
    "            desc=zip_path,\n",
    "            total=total_size,\n",
    "            unit=\"iB\",\n",
    "            unit_scale=True,\n",
    "            unit_divisor=1024,\n",
    "        ) as bar:\n",
    "            for data in response.iter_content(block_size):\n",
    "                file.write(data)\n",
    "                bar.update(len(data))\n",
    "        if not os.path.exists(extract_path):\n",
    "            os.makedirs(extract_path)\n",
    "        with zipfile.ZipFile(zip_path, \"r\") as zip_ref:\n",
    "            for member in zip_ref.namelist():\n",
    "                member_path = os.path.join(\n",
    "                    extract_path,\n",
    "                    os.path.relpath(\n",
    "                        member, start=os.path.commonpath(zip_ref.namelist())\n",
    "                    ),\n",
    "                )\n",
    "                if member.endswith(\"/\"):\n",
    "                    os.makedirs(member_path, exist_ok=True)\n",
    "                else:\n",
    "                    os.makedirs(os.path.dirname(member_path), exist_ok=True)\n",
    "                    with open(member_path, \"wb\") as f:\n",
    "                        f.write(zip_ref.read(member))\n",
    "        os.remove(zip_path)\n",
    "        print(f\"Download and extraction of {extract_path} complete.\")\n",
    "\n",
    "\n",
    "def download(url, file_path):\n",
    "    if os.path.exists(file_path):\n",
    "        print(f\"File {file_path} already exists.\")\n",
    "    else:\n",
    "        print(f\"Downloading the file {file_path}\")\n",
    "        response = requests.get(url, stream=True)\n",
    "        total_size = int(response.headers.get(\"content-length\", 0))\n",
    "        block_size = 1024\n",
    "\n",
    "        with open(file_path, \"wb\") as file, tqdm(\n",
    "            desc=file_path,\n",
    "            total=total_size,\n",
    "            unit=\"iB\",\n",
    "            unit_scale=True,\n",
    "            unit_divisor=1024,\n",
    "        ) as bar:\n",
    "            for data in response.iter_content(block_size):\n",
    "                file.write(data)\n",
    "                bar.update(len(data))\n",
    "        print(f\"Download of {file_path} complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the Finnish Wikipedia 2017 dataset, Finnish Stopwords, Stemmer and FinnWordNet.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wikipedia dataset\n",
    "wiki_url = \"https://www.kielipankki.fi/download/wikipedia-fi/wikipedia-fi-2017-src/wikipedia-fi-2017-src.zip\"\n",
    "wiki_zip_path = \"wikipedia-fi-2017-src.zip\"\n",
    "wiki_extract_path = \"data/wikipedia-fi-2017-src\"\n",
    "download_and_extract(wiki_url, wiki_zip_path, wiki_extract_path)\n",
    "\n",
    "# Finnish Stopwords\n",
    "stopword_url = \"http://members.unine.ch/jacques.savoy/clef/finnishST.txt\"\n",
    "stopwords_path = \"data/finnishST.txt\"\n",
    "download(stopword_url, stopwords_path)\n",
    "\n",
    "# Finnish Stemmer\n",
    "# stemmer_url = \"http://members.unine.ch/jacques.savoy/clef/finnishStemmer.txt\"\n",
    "# stemmer_path = \"data/finnishStemmer.txt\"\n",
    "# download(stemmer_url, stemmer_path)\n",
    "\n",
    "# FinnWordNet\n",
    "finnwordnet_url = (\n",
    "    \"https://www.kielipankki.fi/download/FinnWordNet/v2.0/FinnWordNet-2.0.zip\"\n",
    ")\n",
    "finnwordnet_zip_path = \"FinnWordNet-2.0.zip\"\n",
    "finnwordnet_extract_path = \"data/FinnWordNet\"\n",
    "download_and_extract(finnwordnet_url, finnwordnet_zip_path, finnwordnet_extract_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m spacy download fi_core_news_md\n",
    "!python -m spacy download fi_core_news_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1:\n",
    "\n",
    "Consider the wordings: “climate change”, “emission”, “resilience”, “sustainability” (need to consider their Finnish translations). Suggest a script where your input each of these wordings and output the corresponding Wikipedia pages, highlighting all the (linked) entities in these pages.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "terms = {\"ilmastonmuutos\", \"päästö\", \"joustavuus\", \"ekologinen kestävyys\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load the extracted Wikipedia dataset\n",
    "def load_wikipedia_dataset(directory):\n",
    "    data = {}\n",
    "    data_titles = {}\n",
    "\n",
    "    # Iterate over all dataset parts\n",
    "    files = [\n",
    "        f\n",
    "        for f in os.listdir(directory)\n",
    "        if f.startswith(\"wiki_part\") and f.endswith(\".VRT\")\n",
    "    ]\n",
    "    for filename in tqdm(files, desc=\"Processing files\"):\n",
    "        if filename.startswith(\"wiki_part\") and filename.endswith(\".VRT\"):\n",
    "            file_path = os.path.join(directory, filename)\n",
    "            # print(f\"Processing file: {file_path}\")\n",
    "\n",
    "            with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "                content = file.read()\n",
    "\n",
    "                # Use regex to find all documents within the <doc>...</doc> tags\n",
    "                docs = re.findall(\n",
    "                    r'<doc id=\"(.*?)\" url=\"(.*?)\" title=\"(.*?)\">(.*?)</doc>',\n",
    "                    content,\n",
    "                    re.DOTALL,\n",
    "                )\n",
    "\n",
    "                # Process each document found\n",
    "                for doc_id, url, title, doc_content in docs:\n",
    "                    data_titles[title.casefold()] = {\n",
    "                        \"id\": doc_id,\n",
    "                    }\n",
    "                    # Only store the data if the title matches one of the specified terms\n",
    "                    if title.casefold() in terms:\n",
    "                        data[title.casefold()] = {\n",
    "                            \"id\": doc_id,\n",
    "                            \"url\": url,\n",
    "                            \"content\": doc_content,\n",
    "                        }\n",
    "    return data, data_titles\n",
    "\n",
    "\n",
    "# Load Finnish stopwords\n",
    "def load_stopwords(file_path):\n",
    "    with open(file_path, \"r\", encoding=\"ISO-8859-1\") as f:\n",
    "        stopwords = f.read().splitlines()\n",
    "        print(f\"Loaded {len(stopwords)} stopwords from {file_path}\")\n",
    "    return stopwords\n",
    "\n",
    "\n",
    "# Function to highlight linked entities\n",
    "def highlight_linked_entities(content):\n",
    "    highlighted_content = re.sub(\n",
    "        r'(<link entity=\")(.*?)(\">)(.*?)(</link>)', r\"\\1\\2\\3**\\4**\\5\", content\n",
    "    )\n",
    "    return highlighted_content\n",
    "\n",
    "\n",
    "# Function to tokenize text into sentences and remove stopwords\n",
    "def remove_stopwords(sentence, stopwords, language=\"finnish\"):\n",
    "    words = word_tokenize(text=sentence, language=language)\n",
    "    filtered_words = [word for word in words if word.lower() not in stopwords]\n",
    "    return \" \".join(filtered_words)\n",
    "\n",
    "\n",
    "# Function to remove '#' from the middle of a word\n",
    "def remove_hash_from_words(text):\n",
    "    return re.sub(r\"(\\w)#(\\w)\", r\"\\1\\2\", text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the wikipedia dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_directory = \"data/wikipedia-fi-2017-src\"  # Path to the extracted dataset\n",
    "wikipedia_data, wikipedia_title_data = load_wikipedia_dataset(dataset_directory)\n",
    "print(f\"number of wikipedia titles\", len(wikipedia_title_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the finnish stopwords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finnish_stopwords = load_stopwords(stopwords_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Highlight the entities in the wikipedia pages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for title, info in wikipedia_data.items():\n",
    "    highlighted_content = highlight_linked_entities(info[\"content\"])\n",
    "    # print(f\"Title: {title}\\nHighlighted Content: {highlighted_content}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the pages for the given wordings and get the sentences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract third column data from sentences\n",
    "def extract_third_column(sentence):\n",
    "    third_column_data = []\n",
    "    lines = sentence.strip().split(\"\\n\")\n",
    "    for line in lines:\n",
    "        if line.strip() and not line.startswith(\"<\"):\n",
    "            columns = line.split(\"\\t\")\n",
    "            if len(columns) > 2:\n",
    "                third_column_data.append(columns[2])\n",
    "    return third_column_data\n",
    "\n",
    "\n",
    "def process_paragraphs(input_text):\n",
    "    # print(f\"Input Text: {input_text}\")\n",
    "    paragraphs = re.findall(r\"<paragraph>(.*?)</paragraph>\", input_text, re.DOTALL)\n",
    "    all_third_column_data = []\n",
    "    # print(f\"Paragraphs: {paragraphs}\")\n",
    "    for paragraph in paragraphs:\n",
    "        sentences = re.findall(r\"<sentence>(.*?)</sentence>\", paragraph, re.DOTALL)\n",
    "        # print(f\"Sentences: {sentences}\")\n",
    "        for sentence in sentences:\n",
    "            # print(f\"Sentence: {sentence}\")\n",
    "            third_column_data = extract_third_column(sentence)\n",
    "            all_third_column_data.append(\" \".join(third_column_data))\n",
    "    return all_third_column_data\n",
    "\n",
    "\n",
    "# print(f\"Wikipedia data: {wikipedia_data['ilmastonmuutos']['content']}\")\n",
    "\n",
    "# Process each item in wikipedia_data\n",
    "for item in wikipedia_data.items():\n",
    "    third_column_data = process_paragraphs(item[1][\"content\"])\n",
    "    # print(f\"Item {item[0]} third column data:\")\n",
    "    for j, data in enumerate(third_column_data):\n",
    "        filtered_sentence = remove_stopwords(data, finnish_stopwords)\n",
    "        # print(f\"Sentence {j+1}: {filtered_sentence}\")\n",
    "        # Save the filtered sentence to a variable if needed\n",
    "        # For example: sentence_var = filtered_sentence\n",
    "    # print(\"\\n\" + \"#\" * 80 + \"\\n\")\n",
    "\n",
    "\n",
    "all_words = {}\n",
    "\n",
    "print(wikipedia_data)\n",
    "for title, document in wikipedia_data.items():\n",
    "    sentences = re.findall(\n",
    "        r\"<sentence>(.*?)</sentence>\", document[\"content\"], re.DOTALL\n",
    "    )\n",
    "    document_words = []\n",
    "    if sentences:\n",
    "        for idx, sentence_content in enumerate(sentences):\n",
    "            # print(f\"Processing sentence {idx + 1}...\")\n",
    "\n",
    "            # Split each sentence into individual lines\n",
    "            lines = sentence_content.strip().split(\"\\n\")\n",
    "\n",
    "            # cleaned_lines = re.sub(r'[^\\w\\s]', '', lines)\n",
    "            # Extract the word (3nd column) from each line\n",
    "            words = [line.split(\"\\t\")[2] for line in lines if line.strip()]\n",
    "            for word in words:\n",
    "                cleaned_word = re.sub(r\"[^\\w\\s]\", \"\", word)\n",
    "                if not cleaned_word or cleaned_word in finnish_stopwords:\n",
    "                    continue\n",
    "                document_words.append(cleaned_word.casefold())\n",
    "\n",
    "    all_words[title] = document_words\n",
    "\n",
    "for title, words in all_words.items():\n",
    "    print(f\"\\nWords for document '{title}': {words}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entities_found = {}\n",
    "\n",
    "for i, topic_words in enumerate(all_words):\n",
    "    found_words = []\n",
    "    for word in all_words[topic_words]:\n",
    "        if word.casefold() in [title.casefold() for title in wikipedia_title_data]:\n",
    "            found_words.append(word.casefold())\n",
    "    entities_found[topic_words] = found_words\n",
    "\n",
    "\n",
    "# Example usage of the entities_found dictionary\n",
    "for topic, found_entities in entities_found.items():\n",
    "    print(f\"Document '{topic}' contains the following entities: {found_entities}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2\n",
    "\n",
    "Assume the content of each webpage is a single document. Use relevant NLTK script to create a corpus constituted of the four document, and appropriate proprocessing and lemmatization, to construct the TfIdfVectorizer of each document and then calculate the cosine similarity of each pair of these documents. Provide the result in a table and comment on the findings.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_second_column(sentence):\n",
    "    second_column_data = []\n",
    "    lines = sentence.strip().split(\"\\n\")\n",
    "    for line in lines:\n",
    "        if line.strip() and not line.startswith(\"<\"):\n",
    "            columns = line.split(\"\\t\")\n",
    "            if len(columns) > 2:\n",
    "                second_column_data.append(columns[1])\n",
    "    return second_column_data\n",
    "\n",
    "\n",
    "def process_sentences(input_text):\n",
    "    paragraphs = re.findall(r\"<paragraph>(.*?)</paragraph>\", input_text, re.DOTALL)\n",
    "    all_second_column_data = []\n",
    "    for paragraph in paragraphs:\n",
    "        sentences = re.findall(r\"<sentence>(.*?)</sentence>\", paragraph, re.DOTALL)\n",
    "        # print(f\"Sentences: {sentences}\")\n",
    "        for sentence in sentences:\n",
    "            # print(f\"Sentence: {sentence}\")\n",
    "            second_column_data = extract_second_column(sentence)\n",
    "            all_second_column_data.append(\" \".join(second_column_data))\n",
    "    return all_second_column_data\n",
    "\n",
    "\n",
    "# Preprocessing function to remove stopwords, stemming, and tokenize the document\n",
    "def preProcess(doc, stopwords):\n",
    "    clean_stopwords = [word.casefold() for word in stopwords]\n",
    "    stemmer = SnowballStemmer(\"finnish\")\n",
    "    sentences = process_sentences(doc)\n",
    "    print(f\"Sentences: {sentences}\")\n",
    "    # sentences = sent_tokenize(text=sentences.casefold(), language='finnish')\n",
    "\n",
    "    tokens = []\n",
    "    for sentence in sentences:\n",
    "        words = word_tokenize(sentence)\n",
    "\n",
    "        words = [stemmer.stem(word) for word in words]\n",
    "        words = [\n",
    "            word for word in words if word.isalpha() and word not in clean_stopwords\n",
    "        ]\n",
    "        words_set = list(set(words))\n",
    "\n",
    "        tokens.extend(words_set)\n",
    "\n",
    "    return \" \".join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the document\n",
    "corpus = []\n",
    "\n",
    "for title, document in wikipedia_data.items():\n",
    "    processed_doc = preProcess(document[\"content\"], finnish_stopwords)\n",
    "    print(f\"Processed Document: {processed_doc}\")\n",
    "    corpus.append(processed_doc)\n",
    "\n",
    "# Print the processed document\n",
    "print(\"\\nCorpus:\")\n",
    "for doc in corpus:\n",
    "    print(doc)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF Vectorization of each document\n",
    "tf = TfidfVectorizer(use_idf=True, min_df=1)\n",
    "tfidf_matrix = tf.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute cosine similarity of each pair of these documents\n",
    "cosine_sim = cosine_similarity(tfidf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Provide the result in a table\n",
    "cosine_sim_df = pd.DataFrame(\n",
    "    cosine_sim,\n",
    "    index=[f\"Doc{i+1}\" for i in range(len(cosine_sim))],\n",
    "    columns=[f\"Doc{i+1}\" for i in range(len(cosine_sim))],\n",
    ")\n",
    "\n",
    "print(cosine_sim_df)\n",
    "\n",
    "# Comment on the findings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3\n",
    "\n",
    "Repeat 2) when the documents are restricted only to clickable entities of each document\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing function to remove stopwords, stemming\n",
    "def preProcess_entites(words, stopwords):\n",
    "    clean_stopwords = [word.casefold() for word in stopwords]\n",
    "    stemmer = SnowballStemmer(\"finnish\")\n",
    "\n",
    "    words = [stemmer.stem(word) for word in words]\n",
    "\n",
    "    words = [word for word in words if word.isalpha() and word not in clean_stopwords]\n",
    "\n",
    "    words = list(set(words))\n",
    "\n",
    "    return \" \".join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the document\n",
    "corpus_entity = []\n",
    "for document, found_entities in entities_found.items():\n",
    "    processed_doc = preProcess_entites(found_entities, finnish_stopwords)\n",
    "    corpus_entity.append(processed_doc)\n",
    "\n",
    "# Print the processed document\n",
    "print(\"\\nCorpus_entity:\")\n",
    "for doc in corpus_entity:\n",
    "    print(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF Vectorization of each document\n",
    "tf = TfidfVectorizer(use_idf=True, min_df=1)\n",
    "tfidf_matrix_entity = tf.fit_transform(corpus_entity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute cosine similarity of each pair of these documents\n",
    "cosine_sim_entity = cosine_similarity(tfidf_matrix_entity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Provide the result in a table\n",
    "cosine_sim_df_entity = pd.DataFrame(\n",
    "    cosine_sim_entity,\n",
    "    index=[f\"Doc{i+1}\" for i in range(len(cosine_sim_entity))],\n",
    "    columns=[f\"Doc{i+1}\" for i in range(len(cosine_sim_entity))],\n",
    ")\n",
    "\n",
    "print(cosine_sim_df_entity)\n",
    "\n",
    "# Comment on the findings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4\n",
    "\n",
    "Write a script that explores the clickable entities in each of the four concepts (exploring only once), and generates new extended vocabulary for each concepts, and compute the new similarity between each pair of concepts using extent of overlapping of overall vocabulary and reduced vocabulary (when restricting the vocabulary to clickable entities). We shall refer to the case where the clickable entities are further explored as extended vocabulary case.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Function to further explore clickable entities and generate extended vocabulary\n",
    "# Function to load the extracted Wikipedia dataset\n",
    "def load_wikipedia_dataset_extended(directory):\n",
    "    data_entity_voc = {}\n",
    "    nodup = {}\n",
    "    categories = [\"ilmastonmuutos\", \"päästö\", \"joustavuus\", \"ekologinen kestävyys\"]\n",
    "    data_entity_voc = {category: [] for category in categories}\n",
    "    for topic, found_entities in entities_found.items():\n",
    "        nodup[topic] = list(set(found_entities))\n",
    "\n",
    "    # Iterate over all dataset parts\n",
    "    files = [\n",
    "        f\n",
    "        for f in os.listdir(directory)\n",
    "        if f.startswith(\"wiki_part\") and f.endswith(\".VRT\")\n",
    "    ]\n",
    "\n",
    "    for filename in tqdm(files, desc=\"Processing files\"):\n",
    "        if filename.startswith(\"wiki_part\") and filename.endswith(\".VRT\"):\n",
    "            file_path = os.path.join(directory, filename)\n",
    "            # print(f\"Processing file: {file_path}\")\n",
    "\n",
    "            with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "                content = file.read()\n",
    "\n",
    "                # Use regex to find all documents within the <doc>...</doc> tags\n",
    "                docs = re.findall(\n",
    "                    r'<doc id=\"(.*?)\" url=\"(.*?)\" title=\"(.*?)\">(.*?)</doc>',\n",
    "                    content,\n",
    "                    re.DOTALL,\n",
    "                )\n",
    "\n",
    "                # Process each document found\n",
    "                for doc_id, url, title, doc_content in docs:\n",
    "                    # Loop through the relevant categories in 'nodup' to reduce repetitive code\n",
    "                    for category in categories:\n",
    "                        # Check if the title matches the category terms in 'nodup'\n",
    "                        if title.casefold() in nodup[category]:\n",
    "                            # Create a dictionary with the document data\n",
    "                            document_data = {\n",
    "                                \"title\": title,\n",
    "                                \"id\": doc_id,\n",
    "                                \"url\": url,\n",
    "                                \"content\": doc_content,\n",
    "                            }\n",
    "\n",
    "                            # Append the document data to the corresponding category in 'data_entity_voc'\n",
    "                            data_entity_voc[category].append(document_data)\n",
    "\n",
    "    return data_entity_voc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "dataset_directory = \"data/wikipedia-fi-2017-src\"  # Path to the extracted dataset\n",
    "documents_extended = load_wikipedia_dataset_extended(dataset_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_extended_words (documents_extended):\n",
    "    all_words_extended = {}\n",
    "\n",
    "    for title, documents in documents_extended.items():\n",
    "        for doc in documents:\n",
    "            sentences = re.findall(\n",
    "                r\"<sentence>(.*?)</sentence>\", doc[\"content\"], re.DOTALL\n",
    "            )\n",
    "            document_words = []\n",
    "            if sentences:\n",
    "                for idx, sentence_content in enumerate(sentences):\n",
    "                    # print(f\"Processing sentence {idx + 1}...\")\n",
    "\n",
    "                    # Split each sentence into individual lines\n",
    "                    lines = sentence_content.strip().split(\"\\n\")\n",
    "\n",
    "                    # cleaned_lines = re.sub(r'[^\\w\\s]', '', lines)\n",
    "                    # Extract the word (3nd column) from each line\n",
    "                    words = [line.split(\"\\t\")[2] for line in lines if line.strip()]\n",
    "                    for word in words:\n",
    "                        cleaned_word = re.sub(r\"[^\\w\\s]\", \"\", word)\n",
    "                        if not cleaned_word or cleaned_word in finnish_stopwords:\n",
    "                            continue\n",
    "                        document_words.append(cleaned_word.casefold())\n",
    "\n",
    "            if title in all_words_extended:\n",
    "                all_words_extended[title].extend(document_words)\n",
    "            else:\n",
    "                # If the title doesn't exist, initialize it with the document_words list\n",
    "                all_words_extended[title] = document_words\n",
    "\n",
    "    return all_words_extended\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_extended = list_extended_words(documents_extended)\n",
    "\n",
    "words_extended_all = {} # extended words for all documents \"ilmastonmuutos\", \"päästö\", \"joustavuus\", \"ekologinen kestävyys\"\n",
    "\n",
    "categories = [\"ilmastonmuutos\", \"päästö\", \"joustavuus\", \"ekologinen kestävyys\"]\n",
    "words_extended_all = {category: [] for category in categories}\n",
    "\n",
    "for title, words in all_words.items():\n",
    "    words_extended_all[title] = all_words[title] + words_extended[title]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing function to remove stopwords, stemming, and tokenize the document\n",
    "def preProcess(doc, stopwords):\n",
    "    clean_stopwords = [word.casefold() for word in stopwords]\n",
    "    stemmer = SnowballStemmer(\"finnish\")\n",
    "    sentences = process_sentences(doc)\n",
    "    print(f\"Sentences: {sentences}\")\n",
    "    # sentences = sent_tokenize(text=sentences.casefold(), language='finnish')\n",
    "\n",
    "    tokens = []\n",
    "    for sentence in sentences:\n",
    "        words = word_tokenize(sentence)\n",
    "\n",
    "        words = [stemmer.stem(word) for word in words]\n",
    "        words = [\n",
    "            word for word in words if word.isalpha() and word not in clean_stopwords\n",
    "        ]\n",
    "        words_set = list(set(words))\n",
    "\n",
    "        tokens.extend(words_set)\n",
    "\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "# Compute similiarity for the extended words\n",
    "def compute_similarity(vocab1, vocab2):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_similarity(words_extended, words_extended_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 5\n",
    "\n",
    "We want to assess the importance of each of the four concepts by the number of clickable entities including in the webpage of the given concept. Write a script that implements this strategy to determine the importance of each concept. Next, we want to use the extended vocabulary by quantifying the importance of individual entity in the original webpage by the number of clickable links it generates. Summarize a table highlighting the order of importance of the four concepts according to each of the original and extended vocabulary.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the occurance of each entity in the entities_found\n",
    "entity_occurence = {}\n",
    "for topic, found_entities in entities_found.items():\n",
    "    entity_occurence[topic] = {}\n",
    "    for entity in found_entities:\n",
    "        if entity in entity_occurence[topic]:\n",
    "            entity_occurence[topic][entity] += 1\n",
    "        else:\n",
    "            entity_occurence[topic][entity] = 1\n",
    "\n",
    "# order the entities by occurance\n",
    "sorted_entities = {}\n",
    "for topic, entities in entity_occurence.items():\n",
    "    sorted_entities[topic] = {\n",
    "        k: v for k, v in sorted(entities.items(), key=lambda item: item[1], reverse=True)\n",
    "    }\n",
    "\n",
    "## print the sorted entities for each topic\n",
    "for topic, entities in sorted_entities.items():\n",
    "    print(f\"Document '{topic}' contains entities:\")\n",
    "    print(entities)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## words_extended_all contains the extended vocabulary for each document\n",
    "## entities_found contains the entities found in each document\n",
    "\n",
    "\n",
    "def count_clickable_links(entities):\n",
    "    return {entity: len(entity) for entity in entities}\n",
    "\n",
    "# Count clickable links for original and extended vocabulary\n",
    "original_counts = {concept: count_clickable_links(entities) for concept, entities in entities_found.items()}\n",
    "extended_counts = {concept: count_clickable_links(entities) for concept, entities in words_extended_all.items()}\n",
    "\n",
    "# Summarize importance\n",
    "original_importance = {concept: sum(counts.values()) for concept, counts in original_counts.items()}\n",
    "extended_importance = {concept: sum(counts.values()) for concept, counts in extended_counts.items()}\n",
    "\n",
    "# Create a DataFrame to summarize the importance\n",
    "importance_df = pd.DataFrame({\n",
    "    'Concept': list(original_importance.keys()),\n",
    "    'Original Importance': list(original_importance.values()),\n",
    "    'Extended Importance': list(extended_importance.values())\n",
    "})\n",
    "\n",
    "# Sort the DataFrame by importance\n",
    "importance_df = importance_df.sort_values(by=['Original Importance', 'Extended Importance'], ascending=False)\n",
    "\n",
    "# Display the table\n",
    "print(importance_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 6\n",
    "\n",
    "We want to assess the similarity between the concepts is reflected in the their webpage content. Use a script to calculate Wu and Palmer WordNet semantic similarity between each pair of the four concepts and then compare result with the Jaccard similarity obtained by both original vocabulary in 2) and extended vocabulary 4), and comment on the compatibility between the semantic similarity and the above constructed Jaccard similarity measures\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finnwordnet_path = \"data/FinnWordNet/dict\"\n",
    "wn = WordNetCorpusReader(finnwordnet_path, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wu and Palmer WordNet Semantic Similarity Calculation\n",
    "terms_list = list(terms)\n",
    "wu_palmer_similarity = []\n",
    "for i in range(len(terms_list)):\n",
    "    row = []\n",
    "    for j in range(len(terms_list)):\n",
    "        if i == j:\n",
    "            row.append(1.0)\n",
    "        else:\n",
    "            synsets_1 = wn.synsets(terms_list[i], lang='fin')\n",
    "            synsets_2 = wn.synsets(terms_list[j], lang='fin')\n",
    "            max_similarity = 0\n",
    "            for syn1 in synsets_1:\n",
    "                for syn2 in synsets_2:\n",
    "                    similarity = syn1.wup_similarity(syn2)\n",
    "                    if similarity and similarity > max_similarity:\n",
    "                        max_similarity = similarity\n",
    "            row.append(max_similarity if max_similarity else 0)\n",
    "    wu_palmer_similarity.append(row)\n",
    "\n",
    "# Provide the result in a table\n",
    "wu_palmer_sim_df = pd.DataFrame(\n",
    "    wu_palmer_similarity,\n",
    "    index=[f\"Concept{i+1}\" for i in range(len(terms_list))],\n",
    "    columns=[f\"Concept{i+1}\" for i in range(len(terms_list))],\n",
    ")\n",
    "\n",
    "print(\"\\nWu and Palmer WordNet Semantic Similarity:\")\n",
    "print(wu_palmer_sim_df)\n",
    "\n",
    "# Jaccard Similarity Calculation for Original and Extended Vocabulary\n",
    "def jaccard_similarity(set1, set2):\n",
    "    intersection = len(set1.intersection(set2))\n",
    "    union = len(set1.union(set2))\n",
    "    return intersection / union if union != 0 else 0\n",
    "\n",
    "# Calculate Jaccard Similarity for Original and Extended Vocabularies\n",
    "jaccard_original = []\n",
    "jaccard_extended = []\n",
    "for i in range(len(terms_list)):\n",
    "    row_original = []\n",
    "    row_extended = []\n",
    "    for j in range(len(terms_list)):\n",
    "        if i == j:\n",
    "            row_original.append(1.0)\n",
    "            row_extended.append(1.0)\n",
    "        else:\n",
    "            row_original.append(jaccard_similarity(all_words[terms_list[i]], all_words[terms_list[j]]))\n",
    "            row_extended.append(jaccard_similarity(words_extended_allombined_vocab[terwords_extended_allombined_vocab[tercab <- task 4\n",
    "    jaccard_original.append(row_original)\n",
    "    jaccard_extended.append(row_extended)\n",
    "\n",
    "# Provide the results in tables\n",
    "jaccard_original_df = pd.DataFrame(\n",
    "    jaccard_original,\n",
    "    index=[f\"Concept{i+1}\" for i in range(len(terms_list))],\n",
    "    columns=[f\"Concept{i+1}\" for i in range(len(terms_list))],\n",
    ")\n",
    "\n",
    "jaccard_extended_df = pd.DataFrame(\n",
    "    jaccard_extended,\n",
    "    index=[f\"Concept{i+1}\" for i in range(len(terms_list))],\n",
    "    columns=[f\"Concept{i+1}\" for i in range(len(terms_list))],\n",
    ")\n",
    "\n",
    "print(\"\\nJaccard Similarity (Original Vocabulary):\")\n",
    "print(jaccard_original_df)\n",
    "print(\"\\nJaccard Similarity (Extended Vocabulary):\")\n",
    "print(jaccard_extended_df)\n",
    "\n",
    "# Comment on Compatibility Between Similarity Measures\n",
    "print(\"\\nComments on Compatibility Between Semantic and Jaccard Similarities:\")\n",
    "for i in range(len(terms_list)):\n",
    "    for j in range(i + 1, len(terms_list)):\n",
    "        print(f\"\\nComparison between Concept{i+1} and Concept{j+1}:\")\n",
    "        print(f\"Wu and Palmer Semantic Similarity: {wu_palmer_sim_df.iloc[i, j]:.2f}\")\n",
    "        print(f\"Jaccard Similarity (Original Vocabulary): {jaccard_original_df.iloc[i, j]:.2f}\")\n",
    "        print(f\"Jaccard Similarity (Extended Vocabulary): {jaccard_extended_df.iloc[i, j]:.2f}\")\n",
    "        if wu_palmer_sim_df.iloc[i, j] > 0.5:\n",
    "            print(\"High semantic similarity is generally compatible with high Jaccard similarity.\")\n",
    "        else:\n",
    "            print(\"Low semantic similarity indicates less compatibility with Jaccard similarity.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 7\n",
    "\n",
    "Now we want to evaluate the emotion of the text in each dataframe. For this purpose, use WordnetAffect (https://github.com/clemtoy/WNAffect), which generates an emotion state and the part of speech tag of each word matched in the lexicon, and then compile the overall output for each concept. Repeat this process for both restricted and extended vocabulary cases, and comment on the compatibility between the emotion gained from the corresponding Wikipedia page and the intuitive emotion from individual concept.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Emotion Analysis Using WordNetAffect\n",
    "wna = WNAffect('wordnet-1.6/', 'wn-domains-3.2/') # NANNE ÄÄ PITÄÄ LATAAJ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to analyze emotions of text using WordNetAffect\n",
    "def analyze_emotions(text):\n",
    "    words = text.split()\n",
    "    emotion_count = defaultdict(int)\n",
    "    for word in words:\n",
    "        # Translate the Finnish word to English\n",
    "        translated_word = translator.translate(word, src='fi', dest='en').text\n",
    "        synsets = wn.synsets(translated_word)\n",
    "        for syn in synsets:\n",
    "            emotion = wna.get_emotion(syn)\n",
    "            if emotion:\n",
    "                emotion_count[emotion] += 1\n",
    "    return emotion_count\n",
    "\n",
    "# Analyze emotions for original and extended vocabulary\n",
    "emotion_results_original = {}\n",
    "emotion_results_extended = {}\n",
    "for term, vocab in all_words.items():\n",
    "    emotion_results_original[term] = analyze_emotions(\" \".join(vocab))\n",
    "for term, vocab in combined_vocab.items():\n",
    "    emotion_results_extended[term] = analyze_emotions(\" \".join(vocab))\n",
    "\n",
    "# Print emotion analysis results\n",
    "print(\"\\nEmotion Analysis (Original Vocabulary):\")\n",
    "for term, emotions in emotion_results_original.items():\n",
    "    print(f\"{term}: {dict(emotions)}\")\n",
    "\n",
    "print(\"\\nEmotion Analysis (Extended Vocabulary):\")\n",
    "for term, emotions in emotion_results_extended.items():\n",
    "    print(f\"{term}: {dict(emotions)}\")\n",
    "\n",
    "# Comment on Compatibility Between Emotion Analysis and Intuitive Emotion\n",
    "print(\"\\nComments on Compatibility Between Emotion Analysis and Intuitive Emotion:\")\n",
    "for term in terms_list:\n",
    "    print(f\"\\nConcept: {term}\")\n",
    "    print(f\"Emotion Analysis (Original Vocabulary): {emotion_results_original[term]}\")\n",
    "    print(f\"Emotion Analysis (Extended Vocabulary): {emotion_results_extended[term]}\")\n",
    "    # Add your own intuitive analysis based on the concept meaning\n",
    "    print(\"The intuitive emotion for this concept aligns/does not align well with the emotion extracted from the Wikipedia page.\")words_extended_all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 8\n",
    "\n",
    "We want to repeat this process for sentiment polarity. For this purpose, use the Vader sentiment https://github.com/cjhutto/vaderSentiment to assess the sentiment of each the Wikipedia page associated to each concept (both with original vocabulary and extended vocabulary, by aggregating the sentiment of individual clickable entity’s page) to compute the sentiment associated with each concept. Comment on the sentiment of each concept in restricted and extended vocabulary and its compatibility with the intuitive sentiment gained from the inherent definition of these concepts.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 9\n",
    "\n",
    "We want to investigate the similarity using the embedding representation. For this purpose, write a program that uses word2vec embedding to evaluate\n",
    "\n",
    "i) the consistency of the Wikipedia page by calculating the cosine similarity between the underlined concept and the average word2vec embedding of every words contained in the page\n",
    "\n",
    "ii) the similarity between the various concepts by computing the cosine similarity between the aggregated embedding vector of the corresponding pages. Comment on the compatibility between webpage content and concept and similarity between the various concepts.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 10\n",
    "\n",
    "Repeat 9) when the extended vocabulary is considered.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 11\n",
    "\n",
    "Use appropriate literature to comment on the findings. Also, identify any additional input that would allow you to further elucidate any of the preceding, and use appropriate literature of corpus linguistic literature to justify your findings and comment on the obtained results. Finally, comment on the limitations and structural weakness of the data processing pipeline.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
