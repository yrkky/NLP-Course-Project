{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import all libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import zipfile\n",
    "import requests\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "import numpy as np\n",
    "import wn\n",
    "from wn.similarity import wup\n",
    "from tqdm import tqdm\n",
    "from nltk.corpus.reader.wordnet import WordNetCorpusReader\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.preprocessing import normalize\n",
    "from deep_translator import GoogleTranslator\n",
    "from collections import defaultdict\n",
    "from gensim.models import Word2Vec\n",
    "import gensim\n",
    "import gensim.downloader as api\n",
    "import kagglehub\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "import tarfile\n",
    "from multiprocessing import Pool, Manager\n",
    "import concurrent.futures\n",
    "import pickle\n",
    "import torch\n",
    "from transformers import MarianMTModel, MarianTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Login to Kaggle\n",
    "kagglehub.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Clone WNAffect repository\n",
    "repo_url = \"https://github.com/clemtoy/WNAffect.git\"\n",
    "clone_dir = \"WNAffect\"\n",
    "\n",
    "if not os.path.exists(clone_dir):\n",
    "    os.system(f\"git clone {repo_url} {clone_dir}\")\n",
    "else:\n",
    "    print(\"Failed to clone repository. Directory already exists.\")\n",
    "\n",
    "from WNAffect.wnaffect import WNAffect\n",
    "from WNAffect.emotion import Emotion\n",
    "\n",
    "# Download the nltk data\n",
    "nltk.download(\"wordnet\")\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download(\"omw-1.4\")\n",
    "\n",
    "# Download finnish wordnet\n",
    "!python -m wn download omw-fi\n",
    "\n",
    "# Download finnish word2vec model from kaggle\n",
    "word2vec_fi = kagglehub.dataset_download(\"lehtol/word2vec-fi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_and_extract(url, zip_path, extract_path):\n",
    "    if os.path.exists(extract_path):\n",
    "        print(f\"File {extract_path} already exists.\")\n",
    "    else:\n",
    "        print(f\"Downloading and extracting the {zip_path} to {extract_path}.\")\n",
    "        response = requests.get(url, stream=True)\n",
    "        total_size = int(response.headers.get(\"content-length\", 0))\n",
    "        block_size = 1024\n",
    "        with open(zip_path, \"wb\") as file, tqdm(\n",
    "            desc=zip_path,\n",
    "            total=total_size,\n",
    "            unit=\"iB\",\n",
    "            unit_scale=True,\n",
    "            unit_divisor=1024,\n",
    "        ) as bar:\n",
    "            for data in response.iter_content(block_size):\n",
    "                file.write(data)\n",
    "                bar.update(len(data))\n",
    "        if not os.path.exists(extract_path):\n",
    "            os.makedirs(extract_path)\n",
    "        with zipfile.ZipFile(zip_path, \"r\") as zip_ref:\n",
    "            for member in zip_ref.namelist():\n",
    "                member_path = os.path.join(\n",
    "                    extract_path,\n",
    "                    os.path.relpath(\n",
    "                        member, start=os.path.commonpath(zip_ref.namelist())\n",
    "                    ),\n",
    "                )\n",
    "                if member.endswith(\"/\"):\n",
    "                    os.makedirs(member_path, exist_ok=True)\n",
    "                else:\n",
    "                    os.makedirs(os.path.dirname(member_path), exist_ok=True)\n",
    "                    with open(member_path, \"wb\") as f:\n",
    "                        f.write(zip_ref.read(member))\n",
    "        os.remove(zip_path)\n",
    "        print(f\"Download and extraction of {extract_path} complete.\")\n",
    "\n",
    "\n",
    "def download(url, file_path):\n",
    "    if os.path.exists(file_path):\n",
    "        print(f\"File {file_path} already exists.\")\n",
    "    else:\n",
    "        print(f\"Downloading the file {file_path}\")\n",
    "        response = requests.get(url, stream=True)\n",
    "        total_size = int(response.headers.get(\"content-length\", 0))\n",
    "        block_size = 1024\n",
    "\n",
    "        with open(file_path, \"wb\") as file, tqdm(\n",
    "            desc=file_path,\n",
    "            total=total_size,\n",
    "            unit=\"iB\",\n",
    "            unit_scale=True,\n",
    "            unit_divisor=1024,\n",
    "        ) as bar:\n",
    "            for data in response.iter_content(block_size):\n",
    "                file.write(data)\n",
    "                bar.update(len(data))\n",
    "        print(f\"Download of {file_path} complete.\")\n",
    "\n",
    "\n",
    "def extract_wordnet16_tar(tar_path, extract_path):\n",
    "    def is_within_directory(directory, target):\n",
    "        abs_directory = os.path.abspath(directory)\n",
    "        abs_target = os.path.abspath(target)\n",
    "        return os.path.commonpath([abs_directory]) == os.path.commonpath(\n",
    "            [abs_directory, abs_target]\n",
    "        )\n",
    "\n",
    "    def safe_extract(tar, path=\".\", members=None, *, numeric_owner=False):\n",
    "        for member in tar.getmembers():\n",
    "            member_path = os.path.join(path, member.name)\n",
    "            if not is_within_directory(path, member_path):\n",
    "                raise Exception(\"Attempted Path Traversal in Tar File\")\n",
    "        for member in members:\n",
    "            if member.issym() or member.islnk():\n",
    "                continue  # Skip symbolic links\n",
    "            tar.extract(member, path, numeric_owner=numeric_owner, filter=None)\n",
    "\n",
    "    with tarfile.open(tar_path, \"r\") as tar:\n",
    "        members = [m for m in tar.getmembers() if m.name.startswith(\"wordnet-1.6/\")]\n",
    "        for member in members:\n",
    "            member.name = member.name[len(\"wordnet-1.6/\") :]\n",
    "        safe_extract(tar, extract_path, members=members)\n",
    "\n",
    "\n",
    "def download_and_extract_targz(url, gz, path):\n",
    "    if os.path.exists(path):\n",
    "        print(f\"File {path} already exists.\")\n",
    "    else:\n",
    "        print(f\"Downloading and extracting the {gz} to {path}.\")\n",
    "        download(url, gz)\n",
    "        extract_wordnet16_tar(gz, path)\n",
    "        os.remove(gz)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the Finnish Wikipedia 2017 dataset, Finnish Stopwords, Stemmer, FinnWordNet and Word2Vec.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wikipedia dataset\n",
    "wiki_url = \"https://www.kielipankki.fi/download/wikipedia-fi/wikipedia-fi-2017-src/wikipedia-fi-2017-src.zip\"\n",
    "wiki_zip_path = \"wikipedia-fi-2017-src.zip\"\n",
    "wiki_extract_path = \"data/wikipedia-fi-2017-src\"\n",
    "download_and_extract(wiki_url, wiki_zip_path, wiki_extract_path)\n",
    "\n",
    "# Finnish Stopwords\n",
    "stopword_url = \"http://members.unine.ch/jacques.savoy/clef/finnishST.txt\"\n",
    "stopwords_path = \"data/finnishST.txt\"\n",
    "download(stopword_url, stopwords_path)\n",
    "\n",
    "# Finnish Stemmer\n",
    "# stemmer_url = \"http://members.unine.ch/jacques.savoy/clef/finnishStemmer.txt\"\n",
    "# stemmer_path = \"data/finnishStemmer.txt\"\n",
    "# download(stemmer_url, stemmer_path)\n",
    "\n",
    "# FinnWordNet\n",
    "finnwordnet_url = (\n",
    "    \"https://www.kielipankki.fi/download/FinnWordNet/v2.0/FinnWordNet-2.0.zip\"\n",
    ")\n",
    "finnwordnet_zip_path = \"FinnWordNet-2.0.zip\"\n",
    "finnwordnet_extract_path = \"data/FinnWordNet\"\n",
    "download_and_extract(finnwordnet_url, finnwordnet_zip_path, finnwordnet_extract_path)\n",
    "\n",
    "# Wordnet 1.6\n",
    "wordnet16_url = \"https://wordnetcode.princeton.edu/1.6/wn16.unix.tar.gz\"\n",
    "wordnet16_gz = \"wn16.unix.tar.gz\"\n",
    "wordnet16_path = \"data/wn16\"\n",
    "download_and_extract_targz(wordnet16_url, wordnet16_gz, wordnet16_path)\n",
    "\n",
    "# Download Wordnet Affect\n",
    "wordnet_affect_url = \"https://unioulu-my.sharepoint.com/:u:/g/personal/jyrjanai20_student_oulu_fi/EcfjOMVVQ7dIr_lETcSOFo4B5KEtVvB1I5mmWEVxjspDNQ?e=hKkcAj&download=1\"\n",
    "wordnet_affect_zip = \"wndomains32.zip\"\n",
    "wordnet_affect_path = \"data/wndomains32\"\n",
    "download_and_extract(wordnet_affect_url, wordnet_affect_zip, wordnet_affect_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python -m spacy download fi_core_news_md\n",
    "# !python -m spacy download fi_core_news_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1:\n",
    "\n",
    "Consider the wordings: “climate change”, “emission”, “resilience”, “sustainability” (need to consider their Finnish translations). Suggest a script where your input each of these wordings and output the corresponding Wikipedia pages, highlighting all the (linked) entities in these pages.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "terms = {\"ilmastonmuutos\", \"päästö\", \"joustavuus\", \"ekologinen kestävyys\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load the extracted Wikipedia dataset\n",
    "def load_wikipedia_dataset(directory):\n",
    "    data = {}\n",
    "    data_titles = {}\n",
    "\n",
    "    # Iterate over all dataset parts\n",
    "    files = [\n",
    "        f\n",
    "        for f in os.listdir(directory)\n",
    "        if f.startswith(\"wiki_part\") and f.endswith(\".VRT\")\n",
    "    ]\n",
    "    for filename in tqdm(files, desc=\"Processing files\"):\n",
    "        if filename.startswith(\"wiki_part\") and filename.endswith(\".VRT\"):\n",
    "            file_path = os.path.join(directory, filename)\n",
    "            # print(f\"Processing file: {file_path}\")\n",
    "\n",
    "            with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "                content = file.read()\n",
    "\n",
    "                # Use regex to find all documents within the <doc>...</doc> tags\n",
    "                docs = re.findall(\n",
    "                    r'<doc id=\"(.*?)\" url=\"(.*?)\" title=\"(.*?)\">(.*?)</doc>',\n",
    "                    content,\n",
    "                    re.DOTALL,\n",
    "                )\n",
    "\n",
    "                # Process each document found\n",
    "                for doc_id, url, title, doc_content in docs:\n",
    "                    data_titles[title.casefold()] = {\n",
    "                        \"id\": doc_id,\n",
    "                    }\n",
    "                    # Only store the data if the title matches one of the specified terms\n",
    "                    if title.casefold() in terms:\n",
    "                        data[title.casefold()] = {\n",
    "                            \"id\": doc_id,\n",
    "                            \"url\": url,\n",
    "                            \"content\": doc_content,\n",
    "                        }\n",
    "    return data, data_titles\n",
    "\n",
    "\n",
    "# Load Finnish stopwords\n",
    "def load_stopwords(file_path):\n",
    "    with open(file_path, \"r\", encoding=\"ISO-8859-1\") as f:\n",
    "        stopwords = f.read().splitlines()\n",
    "        print(f\"Loaded {len(stopwords)} stopwords from {file_path}\")\n",
    "    return stopwords\n",
    "\n",
    "\n",
    "# Function to highlight linked entities\n",
    "def highlight_linked_entities(content):\n",
    "    highlighted_content = re.sub(\n",
    "        r'(<link entity=\")(.*?)(\">)(.*?)(</link>)', r\"\\1\\2\\3**\\4**\\5\", content\n",
    "    )\n",
    "    return highlighted_content\n",
    "\n",
    "\n",
    "# Function to tokenize text into sentences and remove stopwords\n",
    "def remove_stopwords(sentence, stopwords, language=\"finnish\"):\n",
    "    words = word_tokenize(text=sentence, language=language)\n",
    "    filtered_words = [word for word in words if word.lower() not in stopwords]\n",
    "    return \" \".join(filtered_words)\n",
    "\n",
    "\n",
    "# Function to remove '#' from the middle of a word\n",
    "def remove_hash_from_words(text):\n",
    "    return re.sub(r\"(\\w)#(\\w)\", r\"\\1\\2\", text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the wikipedia dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_directory = \"data/wikipedia-fi-2017-src\"  # Path to the extracted dataset\n",
    "wikipedia_data, wikipedia_title_data = load_wikipedia_dataset(dataset_directory)\n",
    "print(f\"number of wikipedia titles\", len(wikipedia_title_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the finnish stopwords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finnish_stopwords = load_stopwords(stopwords_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Highlight the entities in the wikipedia pages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "for title, info in wikipedia_data.items():\n",
    "    highlighted_content = highlight_linked_entities(info[\"content\"])\n",
    "    # print(f\"Title: {title}\\nHighlighted Content: {highlighted_content}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the pages for the given wordings and get the sentences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract third column data from sentences\n",
    "def extract_third_column(sentence):\n",
    "    third_column_data = []\n",
    "    lines = sentence.strip().split(\"\\n\")\n",
    "    for line in lines:\n",
    "        if line.strip() and not line.startswith(\"<\"):\n",
    "            columns = line.split(\"\\t\")\n",
    "            if len(columns) > 2:\n",
    "                third_column_data.append(columns[2])\n",
    "    return third_column_data\n",
    "\n",
    "\n",
    "def process_paragraphs(input_text):\n",
    "    # print(f\"Input Text: {input_text}\")\n",
    "    paragraphs = re.findall(r\"<paragraph>(.*?)</paragraph>\", input_text, re.DOTALL)\n",
    "    all_third_column_data = []\n",
    "    # print(f\"Paragraphs: {paragraphs}\")\n",
    "    for paragraph in paragraphs:\n",
    "        sentences = re.findall(r\"<sentence>(.*?)</sentence>\", paragraph, re.DOTALL)\n",
    "        # print(f\"Sentences: {sentences}\")\n",
    "        for sentence in sentences:\n",
    "            # print(f\"Sentence: {sentence}\")\n",
    "            third_column_data = extract_third_column(sentence)\n",
    "            all_third_column_data.append(\" \".join(third_column_data))\n",
    "    return all_third_column_data\n",
    "\n",
    "\n",
    "# print(f\"Wikipedia data: {wikipedia_data['ilmastonmuutos']['content']}\")\n",
    "\n",
    "# Process each item in wikipedia_data\n",
    "for item in wikipedia_data.items():\n",
    "    third_column_data = process_paragraphs(item[1][\"content\"])\n",
    "    # print(f\"Item {item[0]} third column data:\")\n",
    "    for j, data in enumerate(third_column_data):\n",
    "        filtered_sentence = remove_stopwords(data, finnish_stopwords)\n",
    "        # print(f\"Sentence {j+1}: {filtered_sentence}\")\n",
    "        # Save the filtered sentence to a variable if needed\n",
    "        # For example: sentence_var = filtered_sentence\n",
    "    # print(\"\\n\" + \"#\" * 80 + \"\\n\")\n",
    "\n",
    "\n",
    "all_words = {}\n",
    "\n",
    "print(wikipedia_data)\n",
    "for title, document in wikipedia_data.items():\n",
    "    sentences = re.findall(\n",
    "        r\"<sentence>(.*?)</sentence>\", document[\"content\"], re.DOTALL\n",
    "    )\n",
    "    document_words = []\n",
    "    if sentences:\n",
    "        for idx, sentence_content in enumerate(sentences):\n",
    "            # print(f\"Processing sentence {idx + 1}...\")\n",
    "\n",
    "            # Split each sentence into individual lines\n",
    "            lines = sentence_content.strip().split(\"\\n\")\n",
    "\n",
    "            # cleaned_lines = re.sub(r'[^\\w\\s]', '', lines)\n",
    "            # Extract the word (3nd column) from each line\n",
    "            words = [line.split(\"\\t\")[2] for line in lines if line.strip()]\n",
    "            for word in words:\n",
    "                cleaned_word = re.sub(r\"[^\\w\\s]\", \"\", word)\n",
    "                if not cleaned_word or cleaned_word in finnish_stopwords:\n",
    "                    continue\n",
    "                document_words.append(cleaned_word.casefold())\n",
    "\n",
    "    all_words[title] = document_words\n",
    "\n",
    "for title, words in all_words.items():\n",
    "    print(f\"\\nWords for document '{title}': {words}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entities_found = {}\n",
    "\n",
    "for i, topic_words in enumerate(all_words):\n",
    "    found_words = []\n",
    "    for word in all_words[topic_words]:\n",
    "        if word.casefold() in [title.casefold() for title in wikipedia_title_data]:\n",
    "            found_words.append(word.casefold())\n",
    "    entities_found[topic_words] = found_words\n",
    "\n",
    "\n",
    "# Example usage of the entities_found dictionary\n",
    "for topic, found_entities in entities_found.items():\n",
    "    print(f\"Document '{topic}' contains the following entities: {found_entities}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2\n",
    "\n",
    "Assume the content of each webpage is a single document. Use relevant NLTK script to create a corpus constituted of the four document, and appropriate proprocessing and lemmatization, to construct the TfIdfVectorizer of each document and then calculate the cosine similarity of each pair of these documents. Provide the result in a table and comment on the findings.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def extract_second_column(sentence):\n",
    "#     second_column_data = []\n",
    "#     lines = sentence.strip().split(\"\\n\")\n",
    "#     for line in lines:\n",
    "#         if line.strip() and not line.startswith(\"<\"):\n",
    "#             columns = line.split(\"\\t\")\n",
    "#             if len(columns) > 2:\n",
    "#                 second_column_data.append(columns[1])\n",
    "#     return second_column_data\n",
    "\n",
    "\n",
    "# def process_sentences(input_text):\n",
    "#     paragraphs = re.findall(r\"<paragraph>(.*?)</paragraph>\", input_text, re.DOTALL)\n",
    "#     all_second_column_data = []\n",
    "#     for paragraph in paragraphs:\n",
    "#         sentences = re.findall(r\"<sentence>(.*?)</sentence>\", paragraph, re.DOTALL)\n",
    "#         # print(f\"Sentences: {sentences}\")\n",
    "#         for sentence in sentences:\n",
    "#             # print(f\"Sentence: {sentence}\")\n",
    "#             second_column_data = extract_second_column(sentence)\n",
    "#             all_second_column_data.append(\" \".join(second_column_data))\n",
    "#     return all_second_column_data\n",
    "\n",
    "\n",
    "# Preprocessing function to remove stopwords, stemming, and tokenize the document\n",
    "def preProcess(words, stopwords):\n",
    "    tokens = []\n",
    "    clean_stopwords = [word.casefold() for word in stopwords]\n",
    "    stemmer = SnowballStemmer(\"finnish\")\n",
    "    # sentences = process_sentences(doc)\n",
    "    # print(f\"Sentences: {sentences}\")\n",
    "    # sentences = sent_tokenize(text=sentences.casefold(), language='finnish')\n",
    "\n",
    "    # words = word_tokenize(words)\n",
    "\n",
    "    words = [stemmer.stem(word) for word in words]\n",
    "    words = [word for word in words if word.isalpha() and word not in clean_stopwords]\n",
    "    words_set = list(set(words))\n",
    "\n",
    "    tokens.extend(words_set)\n",
    "\n",
    "    return \" \".join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the document\n",
    "corpus = []\n",
    "\n",
    "for title, words in all_words.items():\n",
    "    processed_doc = preProcess(words, finnish_stopwords)\n",
    "    print(f\"Processed Document: {processed_doc}\")\n",
    "    corpus.append(processed_doc)\n",
    "\n",
    "# Print the processed document\n",
    "print(\"\\nCorpus:\")\n",
    "for doc in corpus:\n",
    "    print(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF Vectorization of each document\n",
    "tf = TfidfVectorizer(use_idf=True, min_df=1)\n",
    "tfidf_matrix = tf.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute cosine similarity of each pair of these documents\n",
    "cosine_sim = cosine_similarity(tfidf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Provide the result in a table\n",
    "cosine_sim_df = pd.DataFrame(\n",
    "    cosine_sim,\n",
    "    index=[f\"Doc{i+1}\" for i in range(len(cosine_sim))],\n",
    "    columns=[f\"Doc{i+1}\" for i in range(len(cosine_sim))],\n",
    ")\n",
    "\n",
    "print(cosine_sim_df)\n",
    "\n",
    "# Comment on the findings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3\n",
    "\n",
    "Repeat 2) when the documents are restricted only to clickable entities of each document\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing function to remove stopwords, stemming\n",
    "def preProcess_entites(words, stopwords):\n",
    "    clean_stopwords = [word.casefold() for word in stopwords]\n",
    "    stemmer = SnowballStemmer(\"finnish\")\n",
    "\n",
    "    words = [stemmer.stem(word) for word in words]\n",
    "\n",
    "    words = [word for word in words if word.isalpha() and word not in clean_stopwords]\n",
    "\n",
    "    words = list(set(words))\n",
    "\n",
    "    return \" \".join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the document\n",
    "corpus_entity = []\n",
    "for document, found_entities in entities_found.items():\n",
    "    processed_doc = preProcess_entites(found_entities, finnish_stopwords)\n",
    "    corpus_entity.append(processed_doc)\n",
    "\n",
    "# Print the processed document\n",
    "print(\"\\nCorpus_entity:\")\n",
    "for doc in corpus_entity:\n",
    "    print(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF Vectorization of each document\n",
    "tf = TfidfVectorizer(use_idf=True, min_df=1)\n",
    "tfidf_matrix_entity = tf.fit_transform(corpus_entity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute cosine similarity of each pair of these documents\n",
    "cosine_sim_entity = cosine_similarity(tfidf_matrix_entity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Provide the result in a table\n",
    "cosine_sim_df_entity = pd.DataFrame(\n",
    "    cosine_sim_entity,\n",
    "    index=[f\"Doc{i+1}\" for i in range(len(cosine_sim_entity))],\n",
    "    columns=[f\"Doc{i+1}\" for i in range(len(cosine_sim_entity))],\n",
    ")\n",
    "\n",
    "print(cosine_sim_df_entity)\n",
    "\n",
    "# Comment on the findings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4\n",
    "\n",
    "Write a script that explores the clickable entities in each of the four concepts (exploring only once), and generates new extended vocabulary for each concepts, and compute the new similarity between each pair of concepts using extent of overlapping of overall vocabulary and reduced vocabulary (when restricting the vocabulary to clickable entities). We shall refer to the case where the clickable entities are further explored as extended vocabulary case.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to further explore clickable entities and generate extended vocabulary\n",
    "# Function to load the extracted Wikipedia dataset\n",
    "def load_wikipedia_dataset_extended(directory):\n",
    "    data_entity_voc = {}\n",
    "    nodup = {}\n",
    "    categories = [\"ilmastonmuutos\", \"päästö\", \"joustavuus\", \"ekologinen kestävyys\"]\n",
    "    data_entity_voc = {category: [] for category in categories}\n",
    "    for topic, found_entities in entities_found.items():\n",
    "        nodup[topic] = list(set(found_entities))\n",
    "\n",
    "    # Iterate over all dataset parts\n",
    "    files = [\n",
    "        f\n",
    "        for f in os.listdir(directory)\n",
    "        if f.startswith(\"wiki_part\") and f.endswith(\".VRT\")\n",
    "    ]\n",
    "\n",
    "    for filename in tqdm(files, desc=\"Processing files\"):\n",
    "        if filename.startswith(\"wiki_part\") and filename.endswith(\".VRT\"):\n",
    "            file_path = os.path.join(directory, filename)\n",
    "            # print(f\"Processing file: {file_path}\")\n",
    "\n",
    "            with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "                content = file.read()\n",
    "\n",
    "                # Use regex to find all documents within the <doc>...</doc> tags\n",
    "                docs = re.findall(\n",
    "                    r'<doc id=\"(.*?)\" url=\"(.*?)\" title=\"(.*?)\">(.*?)</doc>',\n",
    "                    content,\n",
    "                    re.DOTALL,\n",
    "                )\n",
    "\n",
    "                # Process each document found\n",
    "                for doc_id, url, title, doc_content in docs:\n",
    "                    # Loop through the relevant categories in 'nodup' to reduce repetitive code\n",
    "                    for category in categories:\n",
    "                        # Check if the title matches the category terms in 'nodup'\n",
    "                        if title.casefold() in nodup[category]:\n",
    "                            # Create a dictionary with the document data\n",
    "                            document_data = {\n",
    "                                \"title\": title,\n",
    "                                \"id\": doc_id,\n",
    "                                \"url\": url,\n",
    "                                \"content\": doc_content,\n",
    "                            }\n",
    "\n",
    "                            # Append the document data to the corresponding category in 'data_entity_voc'\n",
    "                            data_entity_voc[category].append(document_data)\n",
    "\n",
    "    return data_entity_voc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "dataset_directory = \"data/wikipedia-fi-2017-src\"  # Path to the extracted dataset\n",
    "documents_extended = load_wikipedia_dataset_extended(dataset_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_extended_words(documents_extended):\n",
    "    all_words_extended = {}\n",
    "\n",
    "    for title, documents in documents_extended.items():\n",
    "        for doc in documents:\n",
    "            sentences = re.findall(\n",
    "                r\"<sentence>(.*?)</sentence>\", doc[\"content\"], re.DOTALL\n",
    "            )\n",
    "            document_words = []\n",
    "            if sentences:\n",
    "                for idx, sentence_content in enumerate(sentences):\n",
    "                    # print(f\"Processing sentence {idx + 1}...\")\n",
    "\n",
    "                    # Split each sentence into individual lines\n",
    "                    lines = sentence_content.strip().split(\"\\n\")\n",
    "\n",
    "                    # cleaned_lines = re.sub(r'[^\\w\\s]', '', lines)\n",
    "                    # Extract the word (3nd column) from each line\n",
    "                    words = [line.split(\"\\t\")[2] for line in lines if line.strip()]\n",
    "                    for word in words:\n",
    "                        cleaned_word = re.sub(r\"[^\\w\\s]\", \"\", word)\n",
    "                        if not cleaned_word or cleaned_word in finnish_stopwords:\n",
    "                            continue\n",
    "                        document_words.append(cleaned_word.casefold())\n",
    "\n",
    "            if title in all_words_extended:\n",
    "                all_words_extended[title].extend(document_words)\n",
    "            else:\n",
    "                # If the title doesn't exist, initialize it with the document_words list\n",
    "                all_words_extended[title] = document_words\n",
    "\n",
    "    return all_words_extended"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_extended = list_extended_words(documents_extended)\n",
    "\n",
    "words_extended_all = (\n",
    "    {}\n",
    ")  # extended words for all documents \"ilmastonmuutos\", \"päästö\", \"joustavuus\", \"ekologinen kestävyys\"\n",
    "\n",
    "categories = [\"ilmastonmuutos\", \"päästö\", \"joustavuus\", \"ekologinen kestävyys\"]\n",
    "words_extended_all = {category: [] for category in categories}\n",
    "\n",
    "for title, words in all_words.items():\n",
    "    words_extended_all[title] = all_words[title] + words_extended[title]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing function to remove stopwords, stemming\n",
    "def preProcess_extended_words(words, stopwords):\n",
    "    clean_stopwords = [word.casefold() for word in stopwords]\n",
    "    stemmer = SnowballStemmer(\"finnish\")\n",
    "\n",
    "    words = [stemmer.stem(word) for word in words]\n",
    "\n",
    "    words = [word for word in words if word.isalpha() and word not in clean_stopwords]\n",
    "\n",
    "    words = list(set(words))\n",
    "\n",
    "    return \" \".join(words)\n",
    "\n",
    "\n",
    "# Compute similiarity for the extended words\n",
    "\n",
    "\n",
    "def compute_similarity(vocab1, vocab2):\n",
    "    # Join vocabularies into a single string for each concept\n",
    "    text1 = \" \".join(vocab1)\n",
    "    text2 = \" \".join(vocab2)\n",
    "\n",
    "    # Use TfidfVectorizer to create a matrix of the text\n",
    "    tf = TfidfVectorizer(use_idf=True, min_df=1)\n",
    "    tfidf_matrix = tf.fit_transform([text1, text2])\n",
    "\n",
    "    # Compute cosine similarit\n",
    "    cosine_sim = cosine_similarity(tfidf_matrix[0:1], tfidf_matrix[1:2])[0][0]\n",
    "\n",
    "    return cosine_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the extended words + original words and extended words\n",
    "\n",
    "words_extended_preproces = {}\n",
    "words_extended_all_preproces = {}\n",
    "\n",
    "categories = [\"ilmastonmuutos\", \"päästö\", \"joustavuus\", \"ekologinen kestävyys\"]\n",
    "words_extended_preproces = {category: [] for category in categories}\n",
    "words_extended_all_preproces = {category: [] for category in categories}\n",
    "\n",
    "for document, words in words_extended.items():\n",
    "    processed_doc_ext = preProcess_entites(words, finnish_stopwords)\n",
    "    words_extended_preproces[document].append(processed_doc_ext)\n",
    "\n",
    "for document, words in words_extended_all.items():\n",
    "    processed_doc_ext_all = preProcess_entites(words, finnish_stopwords)\n",
    "    words_extended_all_preproces[document].append(processed_doc_ext_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute similarity for the extended words + original words and extended words\n",
    "\n",
    "similarities = {}\n",
    "for title in categories:\n",
    "    similarities[title] = compute_similarity(\n",
    "        words_extended_preproces[title], words_extended_all_preproces[title]\n",
    "    )\n",
    "\n",
    "for title, similarity in similarities.items():\n",
    "    print(\n",
    "        f\"Similarity between the extended vocabulary of '{title}' and the original vocabulary: {similarity}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 5\n",
    "\n",
    "We want to assess the importance of each of the four concepts by the number of clickable entities including in the webpage of the given concept. Write a script that implements this strategy to determine the importance of each concept. Next, we want to use the extended vocabulary by quantifying the importance of individual entity in the original webpage by the number of clickable links it generates. Summarize a table highlighting the order of importance of the four concepts according to each of the original and extended vocabulary.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the occurance of each entity in the entities_found\n",
    "entity_occurence = {}\n",
    "for topic, found_entities in entities_found.items():\n",
    "    entity_occurence[topic] = {}\n",
    "    for entity in found_entities:\n",
    "        if entity in entity_occurence[topic]:\n",
    "            entity_occurence[topic][entity] += 1\n",
    "        else:\n",
    "            entity_occurence[topic][entity] = 1\n",
    "\n",
    "# order the entities by occurance\n",
    "sorted_entities = {}\n",
    "for topic, entities in entity_occurence.items():\n",
    "    sorted_entities[topic] = {\n",
    "        k: v\n",
    "        for k, v in sorted(entities.items(), key=lambda item: item[1], reverse=True)\n",
    "    }\n",
    "\n",
    "## print the sorted entities for each topic\n",
    "for topic, entities in sorted_entities.items():\n",
    "    print(f\"Document '{topic}' contains entities:\")\n",
    "    print(entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## words_extended_all contains the extended vocabulary for each document\n",
    "## entities_found contains the entities found in each document\n",
    "\n",
    "\n",
    "def count_clickable_links(entities):\n",
    "    return {entity: len(entity) for entity in entities}\n",
    "\n",
    "\n",
    "# Count clickable links for original and extended vocabulary\n",
    "original_counts = {\n",
    "    concept: count_clickable_links(entities)\n",
    "    for concept, entities in entities_found.items()\n",
    "}\n",
    "extended_counts = {\n",
    "    concept: count_clickable_links(entities)\n",
    "    for concept, entities in words_extended_all.items()\n",
    "}\n",
    "\n",
    "# Summarize importance\n",
    "original_importance = {\n",
    "    concept: sum(counts.values()) for concept, counts in original_counts.items()\n",
    "}\n",
    "extended_importance = {\n",
    "    concept: sum(counts.values()) for concept, counts in extended_counts.items()\n",
    "}\n",
    "\n",
    "# Create a DataFrame to summarize the importance\n",
    "importance_df = pd.DataFrame(\n",
    "    {\n",
    "        \"Concept\": list(original_importance.keys()),\n",
    "        \"Original Importance\": list(original_importance.values()),\n",
    "        \"Extended Importance\": list(extended_importance.values()),\n",
    "    }\n",
    ")\n",
    "\n",
    "# Sort the DataFrame by importance\n",
    "importance_df = importance_df.sort_values(\n",
    "    by=[\"Original Importance\", \"Extended Importance\"], ascending=False\n",
    ")\n",
    "\n",
    "# Display the table\n",
    "print(importance_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 6\n",
    "\n",
    "We want to assess the similarity between the concepts is reflected in the their webpage content. Use a script to calculate Wu and Palmer WordNet semantic similarity between each pair of the four concepts and then compare result with the Jaccard similarity obtained by both original vocabulary in 2) and extended vocabulary 4), and comment on the compatibility between the semantic similarity and the above constructed Jaccard similarity measures\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finnwordnet_path = \"data/FinnWordNet/dict\"\n",
    "# wn = WordNetCorpusReader(finnwordnet_path, None)\n",
    "\n",
    "fi = wn.Wordnet(\"omw-fi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wu and Palmer WordNet Semantic Similarity Calculation\n",
    "terms_list = list(terms)\n",
    "wu_palmer_similarity = []\n",
    "for i in range(len(terms_list)):\n",
    "    row = []\n",
    "    for j in range(len(terms_list)):\n",
    "        if i == j:\n",
    "            row.append(1.0)\n",
    "        else:\n",
    "            synsets_1 = fi.synsets(terms_list[i])\n",
    "            synsets_2 = fi.synsets(terms_list[j])\n",
    "            max_similarity = 0\n",
    "            for syn1 in synsets_1:\n",
    "                for syn2 in synsets_2:\n",
    "                    similarity = wup(syn1, syn2, simulate_root=True)\n",
    "                    if similarity and similarity > max_similarity:\n",
    "                        max_similarity = similarity\n",
    "            row.append(max_similarity if max_similarity else 0)\n",
    "    wu_palmer_similarity.append(row)\n",
    "\n",
    "# Provide the result in a table\n",
    "wu_palmer_sim_df = pd.DataFrame(\n",
    "    wu_palmer_similarity,\n",
    "    index=[f\"Concept{i+1}\" for i in range(len(terms_list))],\n",
    "    columns=[f\"Concept{i+1}\" for i in range(len(terms_list))],\n",
    ")\n",
    "\n",
    "print(\"\\nWu and Palmer WordNet Semantic Similarity:\")\n",
    "print(wu_palmer_sim_df)\n",
    "\n",
    "\n",
    "# Jaccard Similarity Calculation for Original and Extended Vocabulary\n",
    "def jaccard_similarity(set1, set2):\n",
    "    intersection = len(set(set1).intersection(set(set2)))\n",
    "    union = len(set(set1).union(set(set2)))\n",
    "    return intersection / union if union != 0 else 0\n",
    "\n",
    "\n",
    "# Calculate Jaccard Similarity for Original and Extended Vocabularies\n",
    "jaccard_original = []\n",
    "jaccard_extended = []\n",
    "for i in range(len(terms_list)):\n",
    "    row_original = []\n",
    "    row_extended = []\n",
    "    for j in range(len(terms_list)):\n",
    "        if i == j:\n",
    "            row_original.append(1.0)\n",
    "            row_extended.append(1.0)\n",
    "        else:\n",
    "            row_original.append(\n",
    "                jaccard_similarity(all_words[terms_list[i]], all_words[terms_list[j]])\n",
    "            )\n",
    "            row_extended.append(\n",
    "                jaccard_similarity(\n",
    "                    words_extended_all[terms_list[i]], words_extended_all[terms_list[j]]\n",
    "                )\n",
    "            )\n",
    "    jaccard_original.append(row_original)\n",
    "    jaccard_extended.append(row_extended)\n",
    "\n",
    "# Provide the results in tables\n",
    "jaccard_original_df = pd.DataFrame(\n",
    "    jaccard_original,\n",
    "    index=[f\"Concept{i+1}\" for i in range(len(terms_list))],\n",
    "    columns=[f\"Concept{i+1}\" for i in range(len(terms_list))],\n",
    ")\n",
    "\n",
    "jaccard_extended_df = pd.DataFrame(\n",
    "    jaccard_extended,\n",
    "    index=[f\"Concept{i+1}\" for i in range(len(terms_list))],\n",
    "    columns=[f\"Concept{i+1}\" for i in range(len(terms_list))],\n",
    ")\n",
    "\n",
    "print(\"\\nJaccard Similarity (Original Vocabulary):\")\n",
    "print(jaccard_original_df)\n",
    "print(\"\\nJaccard Similarity (Extended Vocabulary):\")\n",
    "print(jaccard_extended_df)\n",
    "\n",
    "# Comment on Compatibility Between Similarity Measures\n",
    "print(\"\\nComments on Compatibility Between Semantic and Jaccard Similarities:\")\n",
    "for i in range(len(terms_list)):\n",
    "    for j in range(i + 1, len(terms_list)):\n",
    "        print(f\"\\nComparison between Concept{i+1} and Concept{j+1}:\")\n",
    "        print(f\"Wu and Palmer Semantic Similarity: {wu_palmer_sim_df.iloc[i, j]:.2f}\")\n",
    "        print(\n",
    "            f\"Jaccard Similarity (Original Vocabulary): {jaccard_original_df.iloc[i, j]:.2f}\"\n",
    "        )\n",
    "        print(\n",
    "            f\"Jaccard Similarity (Extended Vocabulary): {jaccard_extended_df.iloc[i, j]:.2f}\"\n",
    "        )\n",
    "        if wu_palmer_sim_df.iloc[i, j] > 0.5:\n",
    "            print(\n",
    "                \"High semantic similarity is generally compatible with high Jaccard similarity.\"\n",
    "            )\n",
    "        else:\n",
    "            print(\n",
    "                \"Low semantic similarity indicates less compatibility with Jaccard similarity.\"\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 7\n",
    "\n",
    "Now we want to evaluate the emotion of the text in each dataframe. For this purpose, use WordnetAffect (https://github.com/clemtoy/WNAffect), which generates an emotion state and the part of speech tag of each word matched in the lexicon, and then compile the overall output for each concept. Repeat this process for both restricted and extended vocabulary cases, and comment on the compatibility between the emotion gained from the corresponding Wikipedia page and the intuitive emotion from individual concept.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Emotional Damage          Analysis Using WordNetAffect\n",
    "wna = WNAffect(wordnet16_path, wordnet_affect_path)\n",
    "translator = GoogleTranslator(source=\"fi\", target=\"en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_word(word):\n",
    "    \"\"\"Translate a single word.\"\"\"\n",
    "    return translator.translate(word)\n",
    "\n",
    "\n",
    "def translate_batch(words):\n",
    "    \"\"\"Translate a batch of words.\"\"\"\n",
    "    return translator.translate_batch(words)\n",
    "\n",
    "\n",
    "def load_or_translate(all_words, filename=\"translated_all_words.pkl\"):\n",
    "    \"\"\"Load translated words from disk if available, otherwise translate and save to disk.\"\"\"\n",
    "    if os.path.exists(filename):\n",
    "        with open(filename, \"rb\") as f:\n",
    "            translated_all_words = pickle.load(f)\n",
    "        print(\"Loaded translated words from disk.\")\n",
    "    else:\n",
    "        translated_all_words = {}\n",
    "        for term, vocab in all_words.items():\n",
    "            translated_all_words[term] = translate_batch_marian(vocab)\n",
    "            print(f\"Translated words for '{term}'\")\n",
    "\n",
    "        # translated_all_words = {}\n",
    "        # total_words = sum(len(vocab) for vocab in all_words.values())\n",
    "\n",
    "        # with tqdm(total=total_words, desc=\"Translating words\") as pbar:\n",
    "        #     with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "        #         future_to_word = {executor.submit(translate_word, word): (term, word) for term, vocab in all_words.items() for word in vocab}\n",
    "        #         for future in concurrent.futures.as_completed(future_to_word):\n",
    "        #             term, word = future_to_word[future]\n",
    "        #             translated_word = future.result()\n",
    "        #             if term not in translated_all_words:\n",
    "        #                 translated_all_words[term] = []\n",
    "        #             translated_all_words[term].append(translated_word)\n",
    "        #             pbar.update(1)\n",
    "\n",
    "        with open(filename, \"wb\") as f:\n",
    "            pickle.dump(translated_all_words, f)\n",
    "        print(\"Translated words saved to disk.\")\n",
    "    return translated_all_words\n",
    "\n",
    "\n",
    "# use MarianMTModel for translation\n",
    "def translate_batch_marian(words, model_name=\"Helsinki-NLP/opus-mt-fi-en\"):\n",
    "    print(f\"CUDA: {torch.cuda.is_available()}\")\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Device: {device}\")\n",
    "    torch.cuda.empty_cache()\n",
    "    model = MarianMTModel.from_pretrained(model_name).to(device)\n",
    "    tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
    "    # Generate translation\n",
    "    inputs = tokenizer(words, return_tensors=\"pt\", padding=True).to(device)\n",
    "    # Generate translation\n",
    "    translated = model.generate(**inputs)\n",
    "\n",
    "    # Decode the translated text\n",
    "    translated_text = [\n",
    "        tokenizer.decode(t, skip_special_tokens=True) for t in translated\n",
    "    ]\n",
    "    return translated_text\n",
    "\n",
    "\n",
    "# # Load the model and tokenizer\n",
    "# device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "# model_name = 'Helsinki-NLP/opus-mt-fi-en'\n",
    "# tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
    "# model = MarianMTModel.from_pretrained(model_name).to(device)\n",
    "\n",
    "# # Translate from Finnish to English\n",
    "# src_text = [\"kissa\"]  # Example text in Finnish\n",
    "\n",
    "# # Generate translation\n",
    "# inputs = tokenizer(src_text, return_tensors=\"pt\", padding=True).to(device)\n",
    "\n",
    "# # Generate translation\n",
    "# translated = model.generate(**inputs)\n",
    "\n",
    "# # Decode the translated text\n",
    "# translated_text = [tokenizer.decode(t, skip_special_tokens=True) for t in translated]\n",
    "# print(translated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"CUDA: {torch.cuda.is_available()}\")\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Device: {device}\")\n",
    "\n",
    "\n",
    "def chunks(lst, n):\n",
    "    \"\"\"Yield successive n-sized chunks from lst.\"\"\"\n",
    "    for i in range(0, len(lst), n):\n",
    "        yield lst[i : i + n]\n",
    "\n",
    "\n",
    "def translate_batch_marian(\n",
    "    words, model_name=\"Helsinki-NLP/opus-mt-fi-en\", batch_size=8\n",
    "):\n",
    "    torch.cuda.empty_cache()\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = MarianMTModel.from_pretrained(model_name).to(device)\n",
    "    tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
    "\n",
    "    translations = []\n",
    "    total_words = len(words)\n",
    "    with tqdm(total=total_words, desc=\"Translating batches\") as pbar:\n",
    "        for src_text_list in chunks(words, batch_size):\n",
    "            batch = tokenizer(\n",
    "                src_text_list, return_tensors=\"pt\", padding=True, truncation=True\n",
    "            ).to(device)\n",
    "            gen = model.generate(**batch)\n",
    "            translated_text = tokenizer.batch_decode(gen, skip_special_tokens=True)\n",
    "            translations.extend(translated_text)\n",
    "            pbar.update(len(src_text_list))\n",
    "            # print(f\"Translated {translations} words\")\n",
    "\n",
    "    return translations\n",
    "\n",
    "\n",
    "def load_or_translate(all_words, filename=\"translated_all_words.pkl\", batch_size=128):\n",
    "    \"\"\"Load translated words from disk if available, otherwise translate and save to disk.\"\"\"\n",
    "    if os.path.exists(filename):\n",
    "        with open(filename, \"rb\") as f:\n",
    "            translated_all_words = pickle.load(f)\n",
    "        print(\"Loaded translated words from disk.\")\n",
    "    else:\n",
    "        translated_all_words = {}\n",
    "        for term, vocab in all_words.items():\n",
    "            translated_vocab = translate_batch_marian(vocab, batch_size=batch_size)\n",
    "            translated_all_words[term] = translated_vocab\n",
    "\n",
    "        with open(filename, \"wb\") as f:\n",
    "            pickle.dump(translated_all_words, f)\n",
    "        print(\"Translated words saved to disk.\")\n",
    "    return translated_all_words\n",
    "\n",
    "\n",
    "# Usage\n",
    "translated_all_words = load_or_translate(all_words, \"translated_all_words.pkl\")\n",
    "translated_extended_words = load_or_translate(\n",
    "    words_extended_all, \"translated_extended_words.pkl\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(translated_all_words)\n",
    "print(translated_extended_words)\n",
    "print(len(translated_extended_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usage\n",
    "translated_all_words = load_or_translate(all_words, \"translated_all_words.pkl\")\n",
    "translated_extended_words = load_or_translate(\n",
    "    words_extended_all, \"translated_extended_words.pkl\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Translate the terms in \"words_extended_all\" to English\n",
    "\n",
    "# for term, vocab in words_extended_all.items():\n",
    "# emotion_results_extended[term] = analyze_emotions_v2(\" \".join(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to analyze emotions of text using WordNetAffect\n",
    "def analyze_emotions(text):\n",
    "    words = text.split()\n",
    "    emotion_count = defaultdict(int)\n",
    "    for word in tqdm(words, desc=\"Processing words\"):\n",
    "        pos_tag = nltk.pos_tag([word])[0][1]\n",
    "        synsets = wn.synsets(word, pos=pos_tag)\n",
    "        for syn in synsets:\n",
    "            emotion = wna.get_emotion(syn, pos_tag)\n",
    "            if emotion:\n",
    "                emotion_count[emotion] += 1\n",
    "    return emotion_count\n",
    "\n",
    "\n",
    "#######\n",
    "#######\n",
    "# def process_word(word):\n",
    "#     \"\"\"Process a single word to get its emotion.\"\"\"\n",
    "#     pos_tagged = nltk.pos_tag([word])\n",
    "#     pos = nltk.pos_tag([word])[0][1]\n",
    "#     synsets = wn.synsets(word, pos=pos)\n",
    "#     emotion_count = defaultdict(int)\n",
    "#     for syn in synsets:\n",
    "#         emotion = wna.get_emotion(syn, pos_tagged[0][1])\n",
    "#         if emotion:\n",
    "#             emotion_count[emotion] += 1\n",
    "#     return emotion_count\n",
    "\n",
    "# def analyze_emotions_v2(text):\n",
    "#     words = text.split()\n",
    "#     emotion_count = defaultdict(int)\n",
    "#     manager = Manager()\n",
    "#     counter = manager.Value('i', 0)\n",
    "#     total = len(words)\n",
    "#     with Pool() as pool:\n",
    "#         results = list(tqdm(pool.imap(process_word, [(word, counter, total) for word in words]), total=total, desc=\"Processing words\"))\n",
    "#     for result in results:\n",
    "#         for emotion, count in result.items():\n",
    "#             emotion_count[emotion] += count\n",
    "#     return emotion_count\n",
    "#######\n",
    "#######"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######\n",
    "#######\n",
    "####### USE MULTI-THREADING TO SPEED UP THE PROCESS\n",
    "\n",
    "# # Analyze emotions for original and extended vocabulary\n",
    "# emotion_results_original = {}\n",
    "# emotion_results_extended = {}\n",
    "# for term, vocab in translated_all_words.items():\n",
    "#     emotion_results_original[term] = analyze_emotions_v2(\" \".join(vocab))\n",
    "# for term, vocab in translated_extended_words.items():\n",
    "#     emotion_results_extended[term] = analyze_emotions_v2(\" \".join(vocab))\n",
    "\n",
    "# # Print emotion analysis results\n",
    "# print(\"\\nEmotion Analysis (Original Vocabulary):\")\n",
    "# for term, emotions in emotion_results_original.items():\n",
    "#     print(f\"{term}: {dict(emotions)}\")\n",
    "\n",
    "# print(\"\\nEmotion Analysis (Extended Vocabulary):\")\n",
    "# for term, emotions in emotion_results_extended.items():\n",
    "#     print(f\"{term}: {dict(emotions)}\")\n",
    "\n",
    "# # Comment on Compatibility Between Emotion Analysis and Intuitive Emotion\n",
    "# print(\"\\nComments on Compatibility Between Emotion Analysis and Intuitive Emotion:\")\n",
    "# for term in terms_list:\n",
    "#     print(f\"\\nConcept: {term}\")\n",
    "#     print(f\"Emotion Analysis (Original Vocabulary): {emotion_results_original[term]}\")\n",
    "#     print(f\"Emotion Analysis (Extended Vocabulary): {emotion_results_extended[term]}\")\n",
    "#     # Add your own intuitive analysis based on the concept meaning\n",
    "#     print(\n",
    "#         \"The intuitive emotion for this concept aligns/does not align well with the emotion extracted from the Wikipedia page.\"\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze emotions for original and extended vocabulary\n",
    "emotion_results_original = {}\n",
    "emotion_results_extended = {}\n",
    "for term, vocab in translated_all_words.items():\n",
    "    emotion_results_original[term] = analyze_emotions(\" \".join(vocab))\n",
    "for term, vocab in translated_extended_words.items():\n",
    "    emotion_results_extended[term] = analyze_emotions(\" \".join(vocab))\n",
    "\n",
    "# Print emotion analysis results\n",
    "print(\"\\nEmotion Analysis (Original Vocabulary):\")\n",
    "for term, emotions in emotion_results_original.items():\n",
    "    print(f\"{term}: {dict(emotions)}\")\n",
    "\n",
    "print(\"\\nEmotion Analysis (Extended Vocabulary):\")\n",
    "for term, emotions in emotion_results_extended.items():\n",
    "    print(f\"{term}: {dict(emotions)}\")\n",
    "\n",
    "# Comment on Compatibility Between Emotion Analysis and Intuitive Emotion\n",
    "print(\"\\nComments on Compatibility Between Emotion Analysis and Intuitive Emotion:\")\n",
    "for term in terms_list:\n",
    "    print(f\"\\nConcept: {term}\")\n",
    "    print(f\"Emotion Analysis (Original Vocabulary): {emotion_results_original[term]}\")\n",
    "    print(f\"Emotion Analysis (Extended Vocabulary): {emotion_results_extended[term]}\")\n",
    "    # Add your own intuitive analysis based on the concept meaning\n",
    "    print(\n",
    "        \"The intuitive emotion for this concept aligns/does not align well with the emotion extracted from the Wikipedia page.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 8\n",
    "\n",
    "We want to repeat this process for sentiment polarity. For this purpose, use the Vader sentiment https://github.com/cjhutto/vaderSentiment to assess the sentiment of each the Wikipedia page associated to each concept (both with original vocabulary and extended vocabulary, by aggregating the sentiment of individual clickable entity’s page) to compute the sentiment associated with each concept. Comment on the sentiment of each concept in restricted and extended vocabulary and its compatibility with the intuitive sentiment gained from the inherent definition of these concepts.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Sentiment Analyzer\n",
    "analyzer = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentiment Analysis Using Vader Sentiment\n",
    "# Function to analyze sentiment of text using Vader Sentiment Analyzer\n",
    "def analyze_sentiment(text):\n",
    "    translated_text = translator.translate(text, src=\"fi\", dest=\"en\")\n",
    "    sentiment = analyzer.polarity_scores(translated_text)\n",
    "    return sentiment\n",
    "\n",
    "\n",
    "# Analyze sentiment for original and extended vocabulary\n",
    "sentiment_results_original = {}\n",
    "sentiment_results_extended = {}\n",
    "for term, vocab in all_words.items():\n",
    "    sentiment_results_original[term] = analyze_sentiment(\" \".join(vocab))\n",
    "for term, vocab in words_extended_all.items():\n",
    "    sentiment_results_extended[term] = analyze_sentiment(\" \".join(vocab))\n",
    "\n",
    "# Print sentiment analysis results\n",
    "print(\"\\nSentiment Analysis (Original Vocabulary):\")\n",
    "for term, sentiment in sentiment_results_original.items():\n",
    "    print(f\"{term}: {sentiment}\")\n",
    "\n",
    "print(\"\\nSentiment Analysis (Extended Vocabulary):\")\n",
    "for term, sentiment in sentiment_results_extended.items():\n",
    "    print(f\"{term}: {sentiment}\")\n",
    "\n",
    "# Comment on Compatibility Between Sentiment Analysis and Intuitive Sentiment\n",
    "print(\"\\nComments on Compatibility Between Sentiment Analysis and Intuitive Sentiment:\")\n",
    "for term in terms_list:\n",
    "    print(f\"\\nConcept: {term}\")\n",
    "    print(\n",
    "        f\"Sentiment Analysis (Original Vocabulary): {sentiment_results_original[term]}\"\n",
    "    )\n",
    "    print(\n",
    "        f\"Sentiment Analysis (Extended Vocabulary): {sentiment_results_extended[term]}\"\n",
    "    )\n",
    "    # Add your own intuitive analysis based on the concept meaning\n",
    "    print(\n",
    "        \"The intuitive sentiment for this concept aligns/does not align well with the sentiment extracted from the Wikipedia page.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 9\n",
    "\n",
    "We want to investigate the similarity using the embedding representation. For this purpose, write a program that uses word2vec embedding to evaluate\n",
    "\n",
    "i) the consistency of the Wikipedia page by calculating the cosine similarity between the underlined concept and the average word2vec embedding of every words contained in the page\n",
    "\n",
    "ii) the similarity between the various concepts by computing the cosine similarity between the aggregated embedding vector of the corresponding pages. Comment on the compatibility between webpage content and concept and similarity between the various concepts.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get Word2Vec embedding of a word if it exists in the model\n",
    "def get_word_embedding(word, model):\n",
    "    try:\n",
    "        # print(f\"get_word_embedding: {word}\")\n",
    "        return model[word]\n",
    "    except KeyError:\n",
    "        return None\n",
    "\n",
    "\n",
    "# Function to get average embedding\n",
    "def get_average_embedding(tokens, model):\n",
    "    embeddings = [get_word_embedding(token, model) for token in tokens]\n",
    "    # print(f\"embeddings 1: {embeddings}\")\n",
    "    embeddings = [embedding for embedding in embeddings if embedding is not None]\n",
    "    # print(f\"embeddings 2: {embeddings}\")\n",
    "    if not embeddings:\n",
    "        return None\n",
    "    return np.mean(embeddings, axis=0)\n",
    "\n",
    "\n",
    "# Function to calculate cosine similarity\n",
    "def cosine_similarity_vectors(v1, v2):\n",
    "    v1 = v1.reshape(1, -1)\n",
    "    v2 = v2.reshape(1, -1)\n",
    "    return cosine_similarity(v1, v2)[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word2vec_fi = kagglehub.dataset_download(\"lehtol/word2vec-fi\")\n",
    "def evaluate_page_consistency(page_title, model, stop_words):\n",
    "    # Get the page content\n",
    "    # page_content = load_wikipedia_dataset(dataset_directory, page_title)\n",
    "    # page_content = page_content[\"content\"]\n",
    "\n",
    "    page_content = all_words[page_title]\n",
    "\n",
    "    # Preprocess the page content\n",
    "    page_tokens = preProcess(page_content, stop_words)\n",
    "    page_tokens = word_tokenize(page_tokens)\n",
    "\n",
    "    # Get the embedding of the concept\n",
    "    stemmer = SnowballStemmer(\"finnish\")\n",
    "    word = stemmer.stem(page_title)\n",
    "\n",
    "    concept_embedding = get_word_embedding(word.casefold(), model)\n",
    "    if concept_embedding is None:\n",
    "        print(f\"The concept '{page_title}' is not in the Word2Vec model vocabulary.\")\n",
    "        return\n",
    "\n",
    "    # Get the average embedding of the page content\n",
    "    avg_embedding = get_average_embedding(page_tokens, model)\n",
    "    if avg_embedding is None:\n",
    "        print(\n",
    "            f\"No words in the Wikipedia page '{page_title}' have embeddings in the Word2Vec model.\"\n",
    "        )\n",
    "        return\n",
    "\n",
    "    # Calculate cosine similarity\n",
    "    similarity = cosine_similarity_vectors(concept_embedding, avg_embedding)\n",
    "    print(\n",
    "        f\"Cosine similarity between the page '{page_title}' and average embedding of the page content is: {similarity}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the word2vec model\n",
    "fi_model_path = os.path.join(word2vec_fi, \"word2vec-fi.bin\")\n",
    "word2vec_fi_model = gensim.models.KeyedVectors.load_word2vec_format(\n",
    "    fi_model_path, binary=True\n",
    ")\n",
    "\n",
    "## Evaluate the consistency of the Wikipedia page content with the concept \"ilmastonmuutos\"\n",
    "# evaluate_page_consistency(\n",
    "#     \"ilmastonmuutos\", word2vec_fi_model, finnish_stopwords\n",
    "# )\n",
    "\n",
    "# evaluate_page_consistency(\"kestävyys\", word2vec_fi_model, finnish_stopwords)\n",
    "\n",
    "for title in terms:\n",
    "    evaluate_page_consistency(title, word2vec_fi_model, finnish_stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ii)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get the aggregated embedding of a Wikipedia page\n",
    "def get_page_embedding(page_title, model, stop_words):\n",
    "    # Get the page content\n",
    "    # page_content = load_wikipedia_dataset(dataset_directory, page_title)\n",
    "    # page_content = page_content[\"content\"]\n",
    "\n",
    "    page_content = all_words[page_title]\n",
    "\n",
    "    # Check if the page exists\n",
    "    if page_content is None:\n",
    "        print(f\"The Wikipedia page does not exist.\")\n",
    "        return None\n",
    "\n",
    "    # Preprocess the page content\n",
    "    page_tokens = preProcess(page_content, stop_words)\n",
    "    page_tokens = word_tokenize(page_tokens)\n",
    "\n",
    "    # Get the average embedding of the page content\n",
    "    avg_page_embedding = get_average_embedding(page_tokens, model)\n",
    "\n",
    "    return avg_page_embedding\n",
    "\n",
    "\n",
    "def cosine_similarity_between_pages(page_title_1, page_title_2, model, stop_words):\n",
    "    # Get the aggregated embeddings for both pages\n",
    "    # print(f\"Calculating cosine similarity between '{page_title_1}' and '{page_title_2}'\")\n",
    "    embedding_1 = get_page_embedding(page_title_1, model, stop_words)\n",
    "    embedding_2 = get_page_embedding(page_title_2, model, stop_words)\n",
    "\n",
    "    # print(f\"Embedding 1: {embedding_1}, Embedding 2: {embedding_2}\")\n",
    "\n",
    "    # Check if embeddings exist for both pages\n",
    "    if embedding_1 is None or embedding_2 is None:\n",
    "        print(\"One or both pages do not have valid embeddings.\")\n",
    "        return None\n",
    "\n",
    "    # Reshape the embeddings for cosine similarity calculation\n",
    "    embedding_1 = embedding_1.reshape(1, -1)\n",
    "    embedding_2 = embedding_2.reshape(1, -1)\n",
    "\n",
    "    # Calculate and return the cosine similarity\n",
    "    similarity = cosine_similarity(embedding_1, embedding_2)[0][0]\n",
    "    return similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "terms_list = list(terms)\n",
    "# Calculate the cosine similarity between the pages and print the results in a matrix\n",
    "page_similarity_matrix = np.zeros((len(terms_list), len(terms_list)))\n",
    "for i, page_title_1 in enumerate(terms_list):\n",
    "    for j, page_title_2 in enumerate(terms_list):\n",
    "        similarity = cosine_similarity_between_pages(\n",
    "            page_title_1, page_title_2, word2vec_fi_model, finnish_stopwords\n",
    "        )\n",
    "        page_similarity_matrix[i, j] = similarity\n",
    "\n",
    "# Provide the result in a table\n",
    "page_similarity_df = pd.DataFrame(\n",
    "    page_similarity_matrix,\n",
    "    index=terms_list,\n",
    "    columns=terms_list,\n",
    ")\n",
    "\n",
    "print(\"\\nCosine Similarity Between Wikipedia Pages:\")\n",
    "print(page_similarity_df)\n",
    "\n",
    "# print results in a graph\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(page_similarity_df, annot=True, cmap=\"coolwarm\", fmt=\".2f\")\n",
    "plt.title(\"Cosine Similarity Between Wikipedia Pages\")\n",
    "plt.xlabel(\"Wikipedia Page\")\n",
    "plt.ylabel(\"Wikipedia Page\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 10\n",
    "\n",
    "Repeat 9) when the extended vocabulary is considered.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word2vec_fi = kagglehub.dataset_download(\"lehtol/word2vec-fi\")\n",
    "def evaluate_page_consistency(page_title, model, stop_words):\n",
    "    # Get the page content\n",
    "    page_content = words_extended_all[page_title]\n",
    "\n",
    "    # Preprocess the page content\n",
    "    clean_stopwords = [word.casefold() for word in stop_words]\n",
    "    stemmer = SnowballStemmer(\"finnish\")\n",
    "\n",
    "    page_tokens = [stemmer.stem(word) for word in page_content]\n",
    "\n",
    "    page_tokens = [\n",
    "        word for word in page_tokens if word.isalpha() and word not in clean_stopwords\n",
    "    ]\n",
    "\n",
    "    page_tokens = list(set(page_tokens))\n",
    "\n",
    "    # Get the embedding of the concept\n",
    "    word = stemmer.stem(page_title)\n",
    "\n",
    "    concept_embedding = get_word_embedding(word.casefold(), model)\n",
    "    if concept_embedding is None:\n",
    "        print(f\"The concept '{page_title}' is not in the Word2Vec model vocabulary.\")\n",
    "        return\n",
    "\n",
    "    # Get the average embedding of the page content\n",
    "    avg_embedding = get_average_embedding(page_tokens, model)\n",
    "    if avg_embedding is None:\n",
    "        print(\n",
    "            f\"No words in the Wikipedia page '{page_title}' have embeddings in the Word2Vec model.\"\n",
    "        )\n",
    "        return\n",
    "\n",
    "    # Calculate cosine similarity\n",
    "    similarity = cosine_similarity_vectors(concept_embedding, avg_embedding)\n",
    "    print(\n",
    "        f\"Cosine similarity between the page '{page_title}' and average embedding of the page content is: {similarity}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for title in terms:\n",
    "    evaluate_page_consistency(title, word2vec_fi_model, finnish_stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ii)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get the aggregated embedding of a Wikipedia page\n",
    "def get_page_embedding(page_title, model, stop_words):\n",
    "    # Get the page content\n",
    "\n",
    "    page_content = words_extended_all[page_title]\n",
    "\n",
    "    # Check if the page exists\n",
    "    if page_content is None:\n",
    "        print(f\"The Wikipedia page does not exist.\")\n",
    "        return None\n",
    "\n",
    "    # Preprocess the page content\n",
    "    clean_stopwords = [word.casefold() for word in stop_words]\n",
    "    stemmer = SnowballStemmer(\"finnish\")\n",
    "\n",
    "    page_tokens = [stemmer.stem(word) for word in page_content]\n",
    "\n",
    "    page_tokens = [\n",
    "        word for word in page_tokens if word.isalpha() and word not in clean_stopwords\n",
    "    ]\n",
    "\n",
    "    page_tokens = list(set(page_tokens))\n",
    "\n",
    "    # Get the average embedding of the page content\n",
    "    avg_page_embedding = get_average_embedding(page_tokens, model)\n",
    "\n",
    "    return avg_page_embedding\n",
    "\n",
    "\n",
    "def cosine_similarity_between_pages(page_title_1, page_title_2, model, stop_words):\n",
    "    # Get the aggregated embeddings for both pages\n",
    "    # print(f\"Calculating cosine similarity between '{page_title_1}' and '{page_title_2}'\")\n",
    "    embedding_1 = get_page_embedding(page_title_1, model, stop_words)\n",
    "    embedding_2 = get_page_embedding(page_title_2, model, stop_words)\n",
    "\n",
    "    # print(f\"Embedding 1: {embedding_1}, Embedding 2: {embedding_2}\")\n",
    "\n",
    "    # Check if embeddings exist for both pages\n",
    "    if embedding_1 is None or embedding_2 is None:\n",
    "        print(\"One or both pages do not have valid embeddings.\")\n",
    "        return None\n",
    "\n",
    "    # Reshape the embeddings for cosine similarity calculation\n",
    "    embedding_1 = embedding_1.reshape(1, -1)\n",
    "    embedding_2 = embedding_2.reshape(1, -1)\n",
    "\n",
    "    # Calculate and return the cosine similarity\n",
    "    similarity = cosine_similarity(embedding_1, embedding_2)[0][0]\n",
    "    return similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "terms_list = list(terms)\n",
    "# Calculate the cosine similarity between the pages and print the results in a matrix\n",
    "page_similarity_matrix = np.zeros((len(terms_list), len(terms_list)))\n",
    "for i, page_title_1 in enumerate(terms_list):\n",
    "    for j, page_title_2 in enumerate(terms_list):\n",
    "        similarity = cosine_similarity_between_pages(\n",
    "            page_title_1, page_title_2, word2vec_fi_model, finnish_stopwords\n",
    "        )\n",
    "        page_similarity_matrix[i, j] = similarity\n",
    "\n",
    "# Provide the result in a table\n",
    "page_similarity_df = pd.DataFrame(\n",
    "    page_similarity_matrix,\n",
    "    index=terms_list,\n",
    "    columns=terms_list,\n",
    ")\n",
    "\n",
    "print(\"\\nCosine Similarity Between Wikipedia Pages:\")\n",
    "print(page_similarity_df)\n",
    "\n",
    "# print results in a graph\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(page_similarity_df, annot=True, cmap=\"coolwarm\", fmt=\".2f\")\n",
    "plt.title(\"Cosine Similarity Between Wikipedia Pages\")\n",
    "plt.xlabel(\"Wikipedia Page\")\n",
    "plt.ylabel(\"Wikipedia Page\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 11\n",
    "\n",
    "Use appropriate literature to comment on the findings. Also, identify any additional input that would allow you to further elucidate any of the preceding, and use appropriate literature of corpus linguistic literature to justify your findings and comment on the obtained results. Finally, comment on the limitations and structural weakness of the data processing pipeline.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CUDA",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
